{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introductie Tijdens het achtste semester van de studie ICT & Software Engineering heeft mijn afstudeerstage bij Handpicked Agencies plaatsgevonden. Het project waar ik aan gewerkt heb is de digital twin genaamd Twindle . Dit product is vorig jaar ontwikkeld door Handpicked Labs in samenwerking met Techtenna . Het doel van de applicatie was om de luchtkwaliteit, energieverbruik en brandveiligheid van gebouwen in kaart te brengen door middel van een digitale kopie van het gebouw. Probleemstelling Twindle bepaalt of er voldaan wordt aan de gestelde luchtkwaliteitseisen door de tempratuur, luchtvochtigheid en het CO2-gehalte in een ruimte te meten. Mede door COVID-19 is de luchtkwaliteit in een ruimte steeds belangrijker geworden. Slechte luchtkwaliteit kan ernstige gezondheidsproblemen veroorzaken. Door de luchtkwaliteit te voorspellen en meldingen te geven wanneer deze in gevaar is willen we de leefbaarheid van ruimten verbeteren. Hoofd- en deelvragen Tijdens het gehele project is onderzoek verricht volgens het Development Oriented Triangulation (DOT) framework . Dit framework bestaat uit verschillende onderzoekscategorie\u00ebn die gecombineerd moeten worden om tot een valide conclusie te komen. Om het onderzoek te structureren is er een hoofdvraag en meerdere deelvragen opgesteld. Tijdens de conclusie zullen deze hoofd- en deelvragen beantwoord worden. Hoofdvraag Hoe kan Twindle uitgebreid worden om een hoge luchtkwaliteit in ruimten te garanderen? Deelvragen Onderstaand zijn de deelvragen opgesomd. In het projectplan is een overzicht 1 te vinden waarin wordt toegelicht welke onderzoeksmethoden gebruikt zijn om de vraag te beantwoorden. Wie zijn de gebruikers van Twindle? Welke data is benodigd om luchtkwaliteit te voorspellen? Hoe kan machine learning worden toegepast om luchtkwaliteit te voorspellen? Met welke ethische aspecten dient rekening gehouden te worden? En op welke manier? Hoe kunnen machine learningmodellen gekoppeld worden aan Twindle? Doelstelling Vanaf 18 juni 2021 zullen de gebruikers van Twindle meldingen kunnen ontvangen wanneer slechte luchtkwaliteit wordt verwacht. Het machine learningmodel wat hiervoor wordt toegepast zal een \"recall\" en \"precision\" score hebben van minimaal 90%. Een verdere beschrijving van het doel kan in hoofdstuk 1.2.1 SMART 2 van het projectplan gevonden worden. Behaalde resultaten Uiteindelijk is er een pipeline ontwikkeld die periodiek bekijkt of de machine learningmodellen ge\u00fcpdatet moeten worden en voorspellingen maakt. Via een API is deze applicatie gekoppeld aan Twindle. Onderstaand is een voorbeeld van deze koppeling te zien. Afbeeldling 1: Voorbeeld van koppeling Door deze manier van visualizatie toe te passen worden gebruikers aangespoord om in te grijpen wanneer de situatie dreigt te verslechteren. Leeswijzer Tijdens het afstuderen is gewerkt aan de ICT-competenties zoals beschreven in de HBO-I domeinbeschrijving . In de onderstaande secties zal per competentie kort samengevat worden hoe deze aangetoond is. Analyseren Voordat begonnen was aan het ontwikkelen van het product was het belangrijk om te weten wie er bij betrokken waren en wat zij belangrijk vonden. Hiervoor is tijdens het opstellen van het project is de volgende vraag gesteld: Wie zijn de gebruikers van Twindle? Door middel van een stakeholders-analyse is deze vraag beantwoord. Hierna is verder onderzocht wat de behoeften van deze stakeholders waren door gebruik te maken van story-mappingtechnieken . Daarnaast werd er geanalyseerd welke data de Twindle-applicatie tot nu toe heeft verzameld in een exploratory-data analyse . De resultaten van deze analyses zijn verwerkt in requirements. Deze zijn opgesplitst in de volgende categorie\u00ebn: Software Data Machine learning Deze requirementonderzoeken hebben geleid tot een antwoord op de vraag: Welke data is benodigd om luchtkwaliteit te voorspellen? Ontwerpen Uit de verschillende analyses was gebleken dat het belangrijk is dat personen snel ge\u00efnformeerd worden wanneer de luchtkwaliteit dreigt te verslechteren. De eerste stap hierin is om de meetwaarden die samen de luchtkwaliteit vormen te voorspellen. Hiervoor is de volgende vraag gesteld: Hoe kan machine learning worden toegepast om luchtkwaliteit te voorspellen? Tijdens de modelexperimenten zijn meerdere machine learningtechnieken onderzocht en getest om een antwoord op deze vraag te kunnen geven. De tweede stap was om een systeem te ontwerpen, waardoor de machine learningmodellen up-to-date blijven, periodiek nieuwe voorspellingen maken en deze toegankelijk maken voor, bijvoorbeeld, front-endapplicaties. Dit ontwerp geeft antwoord de de vraag: Hoe kunnen machine learningmodellen gekoppeld worden aan Twindle? Na het ontwerp te hebben ge\u00efmplementeerd was dit opgeleverd . Hieruit bleek dat er meerdere problemen waren met de machine learningmodellen die opgelost moesten worden. Hiervoor is een tweede iteratie van de modelexperimenten uitgevoerd. Realiseren Om het ontwerp te realiseren was iteratief gewerkt. Iedere vier weken heeft er een oplevering plaatsgevonden voor de stakeholders van Handpicked Labs en Techtenna. Tijdens de eerste oplevering was het ontwerp besproken met de stakeholders. Zij waren allemaal akkoord met de beslissingen die daar gemaakt werden. Daarnaast werd voorgesteld om het doel te wijzigen. Tot dit moment in het project was het doel om voor alle servicelagen (luchtkwaliteit, duurzaamheid en brandveiligheid) machine learningmodellen te ontwikkelen. Voorgesteld was om de focus te leggen op de luchtkwaliteit, daar een robuust systeem voor te bouwen en daarna uit te breiden naar de rest van de servicelagen. De stakeholders gingen hiermee akkoord. Hierna was tijdens de tweede oplevering het resultaat van de ARIMA-modellen gepresenteerd. Hier bleken nog enkele problemen mee te zijn: Voorspellingen waren op een te korte termijn Modellen waren te groot (> 1GB) Voorspellingen waren niet accuraat genoeg. Het doel was om deze problemen om te lossen voor de derde oplevering . Hiervoor was een tweede iteratie van de modelexperimenten uitgevoerd wat resulteerde in verbeterde modellen. Deze loste de problemen van de voorgaande oplevering op en de stakeholders waren tevreden over de gemaakte veranderingen. Beheer Om het project in goede banen leiden is tijdens het opstellen van het plan 5 uitgelegd hoe het project beheerd zou worden. In dit onderdeel van het portfolio wordt toegelicht hoe dit in de realiteit verlopen is. Door versie beheer toe te passen wordt het aantal merge conflicten geminimaliseerd. Samen met automatische tests vormt dit de basis voor het continous deployment systeem. Adviseren Na afloop van het project is er gereflecteerd op wat er goed ging en wat er beter kon. Vanuit deze bevindingen zijn adviezen uitgebracht over de volgende onderwerpen Data Workflow Ethiek Door de ethische aspecten van het project te analyseren is antwoord gegeven op de volgende vraag: Met welke ethische aspecten dient rekening gehouden te worden? En op welke manier? Conclusie Ter conclusie zal bepaald worden of aan het doel voldaan is door de hoofd- en deelvragen te beantwoorden en te reflecteren op de stageperiode. Het doel van het project was: Vanaf 18 juni 2021 zullen de gebruikers van Twindle meldingen kunnen ontvangen wanneer slechte luchtkwaliteit wordt verwacht. Het machine learningmodel wat hiervoor wordt toegepast zal een \"recall\" en \"precision\" score hebben van minimaal 90%. Wie zijn de gebruikers van Twindle? Door gebruik te maken van een stakeholdersanalyse is bepaald dat de gebruikers van Twindle in twee groepen vallen; gebouwbeheerders en gebouwgebruikers. Welke data is benodigd om luchtkwaliteit te voorspellen Om te bepalen welke data benodigd is voor de voorspellingen is gezocht naar onderzoeken, die hetzelfde onderwerp betreffen. Hieruit is gebleken dat de oppervlakte van ruimten, het bouwjaar, verwarmingsinstellingen, raamstand, buitentemperatuur, luchtvochtigheid en zonnestraling benodigd zijn. Aangezien deze nog niet aanwezig zijn in de dataset zijn deze verzameld* Hoe kan machinelearning worden toegepast om luchtkwaliteit te voorspellen? Door iteratief modelexperimenten uit te voeren is bevonden dat ridge regression toegepast kan worden om de luchtkwaliteit op korte termijn, tot 15 minuten in de toekomst, te voorspellen. Een model wat op langere termijn effectief is was niet gevonden. De oplossing hiervoor is om gemiddelde waarden voor bepaalde dagen en tijdstippen te gebruiken als voorspelling. Met welke ethische aspecten dient rekening gehouden te worden? En op welke manier? Het Twindleproject wordt al aan hoge standaarden wat betreft privacy en dataverzameling gehouden. Het is belangrijk dat bij uitbreiding van het project, vooral de bij brandveiligheid servicelaag, deze standaarden aangehouden worden. Hoe kunnen machine learningmodellen gekoppeld worden aan Twindle? Een groot gedeelte van de machine learningprojecten faalt omdat de modellen niet goed up-to-date gehouden kunnen worden. Om dit te voorkomen is een pipeline ontwikkeld die periodiek de modellen update en nieuwe voorspellingen maakt. Via een API is dit gekoppeld aan Twindle. *Alleen indien er een model ontwikkeld is voor de desbetreffende meetwaarden Evaluatie & reflectie Om te bepalen of het doel behaald is zal op de STARR-methode 6 gereflecteerd worden op de gehele stageperiode. Situatie Tijdens de periode van 8 februari 2021 tot 9 juli 2021 heb ik stage gelopen bij Handpicked Labs. Het doel moest bereikt zijn op 18 juni 2021. Gedurende deze periode heb ik gewerkt aan het Twindle project. Hierbij werd ik conceptueel begeleid door Sjoerd van Oosten en technisch begeleid door Samet Yilmaz. Taak Het was aan mij om een applicatie te ontwerpen en ontwikkelen waarmee gebruikers van Twindle meldingen konden ontvangen wanneer de luchtkwaliteit in gevaar was. Dit omdat luchtkwaliteit een steeds groter probleem is in ruimten en we personen willen beschermen tegen de gevolgen van slechte luchtkwaliteit. Actie Dit probleem heb ik opgelost door de volgende onderdelen te realiseren: Machine learningmodellen die het CO2-gehalte in een ruimte voorspellen. Een pipeline die deze modellen up-to-date houdt en regelmatig nieuwe voorspellingen maakt. Een front-end uitbreiding waar deze voorspellingen worden gevisualiseerd. Resultaat Deze onderdelen resulteren in een systeem wat nauwkeurige voorspellingen kan maken en deze op een manier visualiseert zodat personen aangespoord worden de situatie te verbeteren. Het andere gedeelte van het doel, het weergeven van meldingen, is wegens tijdgebrek en complicaties tijdens het ontwikkelen niet gerealiseerd. Reflectie Ik vind dat door iteratief te werk gaan feedback te verzamelen en te verwerken er een passende oplossing voor het probleem is gevonden. Wel is het zo dat ik het teleurstellend vind dat het meldingensysteem niet is ge\u00efmplementeerd. Ik denk dat ik te snel aan het ontwikkelen van de machine learningmodellen ben begonnen. Het zou waarschijnlijk beter zijn geweest als ik eerst een proof of concept zou hebben gemaakt waarin de visualisaties en meldingen getoond werden. Vanaf hier had ik sneller de juiste modellen kunnen ontwikkelen. Ook had ik onderschat hoeveel tijd het kostte om de pipline en front-endveranderingen te realiseren. Origineel had ik een sprint ingepland voor het ontwerpen en realiseren van de applicatie. Uiteindelijk heb ik hier ongeveer drie sprints aan besteed. Tijdens de planningsfase van volgende projecten kan ik hier rekening mee houden. Projectplan 1.4.1 & 1.4.2 blz. 13 - 15 \u21a9 Projectplan 1.2.1 SMART blz. 8 - 9 \u21a9 Projectplan 1.4 Onderzoeksvragen blz. 12 - 15 \u21a9 Projectplan 1.5 Eindproducten blz. 16 - 17 \u21a9 Projectplan: 4 Testaanpak en Configuratiemanagement blz. 23 t/m 25 \u21a9 Scribbr: Reflecteren met de STARR-methode \u21a9","title":"Leeswijzer"},{"location":"#introductie","text":"Tijdens het achtste semester van de studie ICT & Software Engineering heeft mijn afstudeerstage bij Handpicked Agencies plaatsgevonden. Het project waar ik aan gewerkt heb is de digital twin genaamd Twindle . Dit product is vorig jaar ontwikkeld door Handpicked Labs in samenwerking met Techtenna . Het doel van de applicatie was om de luchtkwaliteit, energieverbruik en brandveiligheid van gebouwen in kaart te brengen door middel van een digitale kopie van het gebouw.","title":"Introductie"},{"location":"#probleemstelling","text":"Twindle bepaalt of er voldaan wordt aan de gestelde luchtkwaliteitseisen door de tempratuur, luchtvochtigheid en het CO2-gehalte in een ruimte te meten. Mede door COVID-19 is de luchtkwaliteit in een ruimte steeds belangrijker geworden. Slechte luchtkwaliteit kan ernstige gezondheidsproblemen veroorzaken. Door de luchtkwaliteit te voorspellen en meldingen te geven wanneer deze in gevaar is willen we de leefbaarheid van ruimten verbeteren.","title":"Probleemstelling"},{"location":"#hoofd-en-deelvragen","text":"Tijdens het gehele project is onderzoek verricht volgens het Development Oriented Triangulation (DOT) framework . Dit framework bestaat uit verschillende onderzoekscategorie\u00ebn die gecombineerd moeten worden om tot een valide conclusie te komen. Om het onderzoek te structureren is er een hoofdvraag en meerdere deelvragen opgesteld. Tijdens de conclusie zullen deze hoofd- en deelvragen beantwoord worden. Hoofdvraag Hoe kan Twindle uitgebreid worden om een hoge luchtkwaliteit in ruimten te garanderen? Deelvragen Onderstaand zijn de deelvragen opgesomd. In het projectplan is een overzicht 1 te vinden waarin wordt toegelicht welke onderzoeksmethoden gebruikt zijn om de vraag te beantwoorden. Wie zijn de gebruikers van Twindle? Welke data is benodigd om luchtkwaliteit te voorspellen? Hoe kan machine learning worden toegepast om luchtkwaliteit te voorspellen? Met welke ethische aspecten dient rekening gehouden te worden? En op welke manier? Hoe kunnen machine learningmodellen gekoppeld worden aan Twindle?","title":"Hoofd- en deelvragen"},{"location":"#doelstelling","text":"Vanaf 18 juni 2021 zullen de gebruikers van Twindle meldingen kunnen ontvangen wanneer slechte luchtkwaliteit wordt verwacht. Het machine learningmodel wat hiervoor wordt toegepast zal een \"recall\" en \"precision\" score hebben van minimaal 90%. Een verdere beschrijving van het doel kan in hoofdstuk 1.2.1 SMART 2 van het projectplan gevonden worden.","title":"Doelstelling"},{"location":"#behaalde-resultaten","text":"Uiteindelijk is er een pipeline ontwikkeld die periodiek bekijkt of de machine learningmodellen ge\u00fcpdatet moeten worden en voorspellingen maakt. Via een API is deze applicatie gekoppeld aan Twindle. Onderstaand is een voorbeeld van deze koppeling te zien. Afbeeldling 1: Voorbeeld van koppeling Door deze manier van visualizatie toe te passen worden gebruikers aangespoord om in te grijpen wanneer de situatie dreigt te verslechteren.","title":"Behaalde resultaten"},{"location":"#leeswijzer","text":"Tijdens het afstuderen is gewerkt aan de ICT-competenties zoals beschreven in de HBO-I domeinbeschrijving . In de onderstaande secties zal per competentie kort samengevat worden hoe deze aangetoond is.","title":"Leeswijzer"},{"location":"#analyseren","text":"Voordat begonnen was aan het ontwikkelen van het product was het belangrijk om te weten wie er bij betrokken waren en wat zij belangrijk vonden. Hiervoor is tijdens het opstellen van het project is de volgende vraag gesteld: Wie zijn de gebruikers van Twindle? Door middel van een stakeholders-analyse is deze vraag beantwoord. Hierna is verder onderzocht wat de behoeften van deze stakeholders waren door gebruik te maken van story-mappingtechnieken . Daarnaast werd er geanalyseerd welke data de Twindle-applicatie tot nu toe heeft verzameld in een exploratory-data analyse . De resultaten van deze analyses zijn verwerkt in requirements. Deze zijn opgesplitst in de volgende categorie\u00ebn: Software Data Machine learning Deze requirementonderzoeken hebben geleid tot een antwoord op de vraag: Welke data is benodigd om luchtkwaliteit te voorspellen?","title":"Analyseren"},{"location":"#ontwerpen","text":"Uit de verschillende analyses was gebleken dat het belangrijk is dat personen snel ge\u00efnformeerd worden wanneer de luchtkwaliteit dreigt te verslechteren. De eerste stap hierin is om de meetwaarden die samen de luchtkwaliteit vormen te voorspellen. Hiervoor is de volgende vraag gesteld: Hoe kan machine learning worden toegepast om luchtkwaliteit te voorspellen? Tijdens de modelexperimenten zijn meerdere machine learningtechnieken onderzocht en getest om een antwoord op deze vraag te kunnen geven. De tweede stap was om een systeem te ontwerpen, waardoor de machine learningmodellen up-to-date blijven, periodiek nieuwe voorspellingen maken en deze toegankelijk maken voor, bijvoorbeeld, front-endapplicaties. Dit ontwerp geeft antwoord de de vraag: Hoe kunnen machine learningmodellen gekoppeld worden aan Twindle? Na het ontwerp te hebben ge\u00efmplementeerd was dit opgeleverd . Hieruit bleek dat er meerdere problemen waren met de machine learningmodellen die opgelost moesten worden. Hiervoor is een tweede iteratie van de modelexperimenten uitgevoerd.","title":"Ontwerpen"},{"location":"#realiseren","text":"Om het ontwerp te realiseren was iteratief gewerkt. Iedere vier weken heeft er een oplevering plaatsgevonden voor de stakeholders van Handpicked Labs en Techtenna. Tijdens de eerste oplevering was het ontwerp besproken met de stakeholders. Zij waren allemaal akkoord met de beslissingen die daar gemaakt werden. Daarnaast werd voorgesteld om het doel te wijzigen. Tot dit moment in het project was het doel om voor alle servicelagen (luchtkwaliteit, duurzaamheid en brandveiligheid) machine learningmodellen te ontwikkelen. Voorgesteld was om de focus te leggen op de luchtkwaliteit, daar een robuust systeem voor te bouwen en daarna uit te breiden naar de rest van de servicelagen. De stakeholders gingen hiermee akkoord. Hierna was tijdens de tweede oplevering het resultaat van de ARIMA-modellen gepresenteerd. Hier bleken nog enkele problemen mee te zijn: Voorspellingen waren op een te korte termijn Modellen waren te groot (> 1GB) Voorspellingen waren niet accuraat genoeg. Het doel was om deze problemen om te lossen voor de derde oplevering . Hiervoor was een tweede iteratie van de modelexperimenten uitgevoerd wat resulteerde in verbeterde modellen. Deze loste de problemen van de voorgaande oplevering op en de stakeholders waren tevreden over de gemaakte veranderingen.","title":"Realiseren "},{"location":"#beheer","text":"Om het project in goede banen leiden is tijdens het opstellen van het plan 5 uitgelegd hoe het project beheerd zou worden. In dit onderdeel van het portfolio wordt toegelicht hoe dit in de realiteit verlopen is. Door versie beheer toe te passen wordt het aantal merge conflicten geminimaliseerd. Samen met automatische tests vormt dit de basis voor het continous deployment systeem.","title":"Beheer "},{"location":"#adviseren","text":"Na afloop van het project is er gereflecteerd op wat er goed ging en wat er beter kon. Vanuit deze bevindingen zijn adviezen uitgebracht over de volgende onderwerpen Data Workflow Ethiek Door de ethische aspecten van het project te analyseren is antwoord gegeven op de volgende vraag: Met welke ethische aspecten dient rekening gehouden te worden? En op welke manier?","title":"Adviseren "},{"location":"#conclusie","text":"Ter conclusie zal bepaald worden of aan het doel voldaan is door de hoofd- en deelvragen te beantwoorden en te reflecteren op de stageperiode. Het doel van het project was: Vanaf 18 juni 2021 zullen de gebruikers van Twindle meldingen kunnen ontvangen wanneer slechte luchtkwaliteit wordt verwacht. Het machine learningmodel wat hiervoor wordt toegepast zal een \"recall\" en \"precision\" score hebben van minimaal 90%. Wie zijn de gebruikers van Twindle? Door gebruik te maken van een stakeholdersanalyse is bepaald dat de gebruikers van Twindle in twee groepen vallen; gebouwbeheerders en gebouwgebruikers. Welke data is benodigd om luchtkwaliteit te voorspellen Om te bepalen welke data benodigd is voor de voorspellingen is gezocht naar onderzoeken, die hetzelfde onderwerp betreffen. Hieruit is gebleken dat de oppervlakte van ruimten, het bouwjaar, verwarmingsinstellingen, raamstand, buitentemperatuur, luchtvochtigheid en zonnestraling benodigd zijn. Aangezien deze nog niet aanwezig zijn in de dataset zijn deze verzameld* Hoe kan machinelearning worden toegepast om luchtkwaliteit te voorspellen? Door iteratief modelexperimenten uit te voeren is bevonden dat ridge regression toegepast kan worden om de luchtkwaliteit op korte termijn, tot 15 minuten in de toekomst, te voorspellen. Een model wat op langere termijn effectief is was niet gevonden. De oplossing hiervoor is om gemiddelde waarden voor bepaalde dagen en tijdstippen te gebruiken als voorspelling. Met welke ethische aspecten dient rekening gehouden te worden? En op welke manier? Het Twindleproject wordt al aan hoge standaarden wat betreft privacy en dataverzameling gehouden. Het is belangrijk dat bij uitbreiding van het project, vooral de bij brandveiligheid servicelaag, deze standaarden aangehouden worden. Hoe kunnen machine learningmodellen gekoppeld worden aan Twindle? Een groot gedeelte van de machine learningprojecten faalt omdat de modellen niet goed up-to-date gehouden kunnen worden. Om dit te voorkomen is een pipeline ontwikkeld die periodiek de modellen update en nieuwe voorspellingen maakt. Via een API is dit gekoppeld aan Twindle. *Alleen indien er een model ontwikkeld is voor de desbetreffende meetwaarden","title":"Conclusie"},{"location":"#evaluatie-reflectie","text":"Om te bepalen of het doel behaald is zal op de STARR-methode 6 gereflecteerd worden op de gehele stageperiode. Situatie Tijdens de periode van 8 februari 2021 tot 9 juli 2021 heb ik stage gelopen bij Handpicked Labs. Het doel moest bereikt zijn op 18 juni 2021. Gedurende deze periode heb ik gewerkt aan het Twindle project. Hierbij werd ik conceptueel begeleid door Sjoerd van Oosten en technisch begeleid door Samet Yilmaz. Taak Het was aan mij om een applicatie te ontwerpen en ontwikkelen waarmee gebruikers van Twindle meldingen konden ontvangen wanneer de luchtkwaliteit in gevaar was. Dit omdat luchtkwaliteit een steeds groter probleem is in ruimten en we personen willen beschermen tegen de gevolgen van slechte luchtkwaliteit. Actie Dit probleem heb ik opgelost door de volgende onderdelen te realiseren: Machine learningmodellen die het CO2-gehalte in een ruimte voorspellen. Een pipeline die deze modellen up-to-date houdt en regelmatig nieuwe voorspellingen maakt. Een front-end uitbreiding waar deze voorspellingen worden gevisualiseerd. Resultaat Deze onderdelen resulteren in een systeem wat nauwkeurige voorspellingen kan maken en deze op een manier visualiseert zodat personen aangespoord worden de situatie te verbeteren. Het andere gedeelte van het doel, het weergeven van meldingen, is wegens tijdgebrek en complicaties tijdens het ontwikkelen niet gerealiseerd. Reflectie Ik vind dat door iteratief te werk gaan feedback te verzamelen en te verwerken er een passende oplossing voor het probleem is gevonden. Wel is het zo dat ik het teleurstellend vind dat het meldingensysteem niet is ge\u00efmplementeerd. Ik denk dat ik te snel aan het ontwikkelen van de machine learningmodellen ben begonnen. Het zou waarschijnlijk beter zijn geweest als ik eerst een proof of concept zou hebben gemaakt waarin de visualisaties en meldingen getoond werden. Vanaf hier had ik sneller de juiste modellen kunnen ontwikkelen. Ook had ik onderschat hoeveel tijd het kostte om de pipline en front-endveranderingen te realiseren. Origineel had ik een sprint ingepland voor het ontwerpen en realiseren van de applicatie. Uiteindelijk heb ik hier ongeveer drie sprints aan besteed. Tijdens de planningsfase van volgende projecten kan ik hier rekening mee houden. Projectplan 1.4.1 & 1.4.2 blz. 13 - 15 \u21a9 Projectplan 1.2.1 SMART blz. 8 - 9 \u21a9 Projectplan 1.4 Onderzoeksvragen blz. 12 - 15 \u21a9 Projectplan 1.5 Eindproducten blz. 16 - 17 \u21a9 Projectplan: 4 Testaanpak en Configuratiemanagement blz. 23 t/m 25 \u21a9 Scribbr: Reflecteren met de STARR-methode \u21a9","title":"Evaluatie &amp; reflectie"},{"location":"adviseren/","text":"Data Missende gegevens Tijdens de data exploratie was opgemerkt dat niet in iedere ruimte dezelfde data bijgehouden werd, zie tabel 1. Dit heeft ervoor gezorgd dat het niet mogelijk was om voor elke ruimte hetzelfde machine learningmodel te ontwikkelen. Ruimte Fijnstofdeeltjes Luchtdruk CO2 Verlichting Activiteit Temperatuur Luchtvochtigheid Deurstatus* Deurgebruik* Apenrots West X X Apenrots Oost X X The Living X X Classroom X X Toilets X X Boardroom X X X X X X X Tabel 1: Dataverzameling per ruimte in de Big Top *Deurstatus en deurgebruik zijn momenteel geen onderdeel van het machine learningmodel. Een oplossing zou zijn om voor iedere ruimte sensoren te installeren die de waarden meten die in table 1 worden gespecificeerd. Op deze manier kan er voor iedere ruimte een model ontwikkeld worden en zal de applicatie beter toegepast kunnen worden om de luchtkwaliteit te verbeteren. Verzameling De data die beschikbaar was tijdens het project verzameld tussen 2021-01-07 09:11 en 2021-02-09 12:54 . Dit betekend dat er maar ongeveer een maand aan data gebruikt kon worden om de machine learning modellen te trainen. Tijdens het onderzoek van Kallio et al. 1 werd een dataset van een jaar gebruikt. Dit is belangrijk om bepaalde seizoensgebonden trends op te kunnen nemen in het model. De voorgestelde oplossing is om een jaar aan data te verzamelen en daarna de modellen te trainen op de complete dataset. Op deze manier kan gevalideerd worden of de bevindingen van dit project juist waren. Workflow Teststrategie Gedurende het ontwikkelen van de machine learning API is er gewerkt op een test driven manier. Het voordeel hiervan is dat veranderingen die delen de software breken tijdig opgespoord konden worden. In de front-end- en back-endapplicaties van het Twindle project is deze strategie niet toegepast. Hierdoor kon het lastig zijn om te bepalen waar bugs vandaan kwamen. Het voorstel is om in de toekomst een striktere teststrategie te hanteren. Op deze manier zal uitbreiding van bestaande applicaties vloeiender verlopen. Continuous integration & deployment De bedoeling was om tijdens het project gebruik te maken van continous integration (CI) en deployment (CD) . Hiervoor was een sterke teststrategie en versiebeheersysteem toegepast. Daarnaast moet er ook een omgeving zijn om de applicaties te hosten. Deze was er wel maar was niet beschikbaar voor de automatische pipelines. Om in de toekomst betere demo's te kunnen geven en hogere kwaliteit software af te leveren is het advies om een development en release omgeving beschikbaar te stellen voor de stagiaires. Ethiek Om te bepalen met welke ethische aspecten rekening gehouden moet worden en op welke manier is de TICT-tool 2 toegepast. Dit is een tool die helpt bij het inschatten van de impact die een nieuwe technology gaat maken. Voor dit project is gebruik gemaakt van de \"quick scan\" optie om een globaal overzicht van de situatie te krijgen 3 . Belangrijke bevindingen van deze quickscan zijn dat er al veel rekening gehouden wordt met privacy en dat er een positieve impact gemaakt wordt op de personen die de applicatie gebruiken. Er zijn plannen om de applicatie uit te breiden zodat deze de brandveiligheid kan monitoren. Wanneer deze uitbreidingen worden gemaakt is het belangrijk dat er aan dezelfde hoge privacy eisen wordt gehouden. Forecasting office indoor CO2 concentration using machine learning with a one-year dataset \u21a9 Technology Impact Cycle Tool \u21a9 TICT Quickscan \u21a9","title":"Adviseren"},{"location":"adviseren/#data","text":"","title":"Data"},{"location":"adviseren/#missende-gegevens","text":"Tijdens de data exploratie was opgemerkt dat niet in iedere ruimte dezelfde data bijgehouden werd, zie tabel 1. Dit heeft ervoor gezorgd dat het niet mogelijk was om voor elke ruimte hetzelfde machine learningmodel te ontwikkelen. Ruimte Fijnstofdeeltjes Luchtdruk CO2 Verlichting Activiteit Temperatuur Luchtvochtigheid Deurstatus* Deurgebruik* Apenrots West X X Apenrots Oost X X The Living X X Classroom X X Toilets X X Boardroom X X X X X X X Tabel 1: Dataverzameling per ruimte in de Big Top *Deurstatus en deurgebruik zijn momenteel geen onderdeel van het machine learningmodel. Een oplossing zou zijn om voor iedere ruimte sensoren te installeren die de waarden meten die in table 1 worden gespecificeerd. Op deze manier kan er voor iedere ruimte een model ontwikkeld worden en zal de applicatie beter toegepast kunnen worden om de luchtkwaliteit te verbeteren.","title":"Missende gegevens"},{"location":"adviseren/#verzameling","text":"De data die beschikbaar was tijdens het project verzameld tussen 2021-01-07 09:11 en 2021-02-09 12:54 . Dit betekend dat er maar ongeveer een maand aan data gebruikt kon worden om de machine learning modellen te trainen. Tijdens het onderzoek van Kallio et al. 1 werd een dataset van een jaar gebruikt. Dit is belangrijk om bepaalde seizoensgebonden trends op te kunnen nemen in het model. De voorgestelde oplossing is om een jaar aan data te verzamelen en daarna de modellen te trainen op de complete dataset. Op deze manier kan gevalideerd worden of de bevindingen van dit project juist waren.","title":"Verzameling"},{"location":"adviseren/#workflow","text":"","title":"Workflow"},{"location":"adviseren/#teststrategie","text":"Gedurende het ontwikkelen van de machine learning API is er gewerkt op een test driven manier. Het voordeel hiervan is dat veranderingen die delen de software breken tijdig opgespoord konden worden. In de front-end- en back-endapplicaties van het Twindle project is deze strategie niet toegepast. Hierdoor kon het lastig zijn om te bepalen waar bugs vandaan kwamen. Het voorstel is om in de toekomst een striktere teststrategie te hanteren. Op deze manier zal uitbreiding van bestaande applicaties vloeiender verlopen.","title":"Teststrategie"},{"location":"adviseren/#continuous-integration-deployment","text":"De bedoeling was om tijdens het project gebruik te maken van continous integration (CI) en deployment (CD) . Hiervoor was een sterke teststrategie en versiebeheersysteem toegepast. Daarnaast moet er ook een omgeving zijn om de applicaties te hosten. Deze was er wel maar was niet beschikbaar voor de automatische pipelines. Om in de toekomst betere demo's te kunnen geven en hogere kwaliteit software af te leveren is het advies om een development en release omgeving beschikbaar te stellen voor de stagiaires.","title":"Continuous integration &amp; deployment"},{"location":"adviseren/#ethiek","text":"Om te bepalen met welke ethische aspecten rekening gehouden moet worden en op welke manier is de TICT-tool 2 toegepast. Dit is een tool die helpt bij het inschatten van de impact die een nieuwe technology gaat maken. Voor dit project is gebruik gemaakt van de \"quick scan\" optie om een globaal overzicht van de situatie te krijgen 3 . Belangrijke bevindingen van deze quickscan zijn dat er al veel rekening gehouden wordt met privacy en dat er een positieve impact gemaakt wordt op de personen die de applicatie gebruiken. Er zijn plannen om de applicatie uit te breiden zodat deze de brandveiligheid kan monitoren. Wanneer deze uitbreidingen worden gemaakt is het belangrijk dat er aan dezelfde hoge privacy eisen wordt gehouden. Forecasting office indoor CO2 concentration using machine learning with a one-year dataset \u21a9 Technology Impact Cycle Tool \u21a9 TICT Quickscan \u21a9","title":"Ethiek"},{"location":"analyseren/","text":"Stakeholders analyse Om te bepalen welke partijen relevant waren voor het project was een stakeholdersanalyse uitgevoerd 1 . De methodiek om de stakeholdersanalyse uit te voeren bestaat uit het identificeren- , prioriteren- en het begrijpen van deze groepen personen 2 . Identificeren Om de stakeholders te kunnen identificeren was een brainstormsessie gehouden. Het resultaat van deze sessie is een mindmap van alle personen of entiteiten die invloed hebben op het project. In de onderstaande afbeelding is deze mindmap te zien. Afbeelding 1: Stakeholders-mindmap Prioriteren Om de communicatiemethoden te bepalen zijn de stakeholders geprioriteerd. Dit was gedaan door te kijken wat hun niveau van invloed is en hoeveel belang zij hadden bij het project. In het onderstaande \u201cStakeholder Power Interest Grid\u201d is dit gevisualiseerd. Afbeelding 2: Power-interest grid Begrijpen De volgende stap was om, per stakeholder, te begrijpen wat hun relatie was tot het project. Dit was gedaan door per stakeholdercategorie een aantal vragen te stellen en te beantwoorden 3 . Gebaseerd op deze vragen is het onderstaande communicatieplan opgesteld. Afbeelding 3: Communicatie plan Story mapping Gedurende het project was een agile werkwijze worden gehanteerd. Een van de onderdelen hiervan is een backlog waarin alle taken waren vastgelegd. De techniek die gebruikt was voor het opstellen van deze backlog is story mapping 4 . De voorbereiding en conclusie van deze story mappingsessie 5 worden onderstaand toegelicht. Voorbereiding Aan de hand van het communicatieplan, zie afbeelding 3: Communicatie plan was een lijst met personen opgesteld die aanwezig moesten zijn bij de story mappingsessie. Afbeelding 4: Aanwezigen Personas waren toegepast om een duidelijker beeld te krijgen bij de gebruikers van de applicatie. Uiteindelijk was vastgesteld dat er twee gebruikersgroepen binnen de applicatie zijn 6 . Hotelmanagers & consumers. Resultaat De story mapping sessie heeft plaatsgevonden op 02-03-2021. Om iedereen duidelijk te maken wat er verwacht werd was een korte presentatie gegeven. Daarna is de onderstaande storymap opgesteld. Deze is gebruikt om via Trello een backlog op te stellen. Afbeelding 5: Storymap Exploratory Data Analyse De eerste stap om te kunnen bepalen welke machine learningmodellen toegepast konden worden was het analyseren van de data die Twindle tot nu toe verzameld heeft. Hiervoor is een Exploratory Data Analyse 7 (EDA) gemaakt volgens de methode van Sunil Ray 8 . Dit bestaat uit de volgende onderdelen: Variable identificatie Univariate analyse Multivariate analyse Missende gegevens Uitschieters Variable identificatie De volgende gegevens, en hun datatype, waren aanwezig in de dataset. Een overzicht van de handelingen die gedaan waren om de onjuiste datatypes op te lossen kan gevonden worden in hoofdstuk 1.1 van de EDA 9 . TVOC float64 # 'Total volatile organic componds' in PPB Pressure float64 # Luchtdruk in Pa. CO2 float64 # CO2 concentratie in PPM. Illumination float64 # Verlichtingsniveau in flux. Activity float64 # 'Person in Room' (PIR) TimeStamp datetime64[ns] # Datum en tijd van meting TempInt float64 # Temperatuur in graden celsius Humidity float64 # Luchtvochtigheidspercentage DOOR_OPEN_STATUS float64 # Boolean of de deur open of dicht is. DOOR_OPEN_TIMES float64 # Aantal keer dat de deur geopend is. room object # Ruimte waarin de meting is vericht. Univariate analyse Univariate analyse is de eenvoudigste vorm van data analyse. Tijdens deze analyse werd elke variabele afzonderlijk geanalyseerd. Dit werd gedaan door de gegevens in histogrammen en boxplots te visualiseren. Hierdoor konden eventuele afwijkingen of uitschieters gedetecteerd worden. Afbeelding 6: Big Top's Boardroom distributie In de bovenstaande afbeelding zijn de distributies van alle meetwaarden in de boardroom van de Big Top gevisualiseerd. Hierin valt te zien dat de meeste waarden een redelijk normaal gedistribueerd zijn. Sommigen zoals bijvoorbeeld de CO2-gehalte hebben afwijkingen en uitschieters. Tijdens de modelleringfase is hier rekening mee gehouden. Multivariate analyse Tijdens dit soort data analyse zijn de verbanden tussen twee variabelen geanalyseerd. Dit is gedaan correlatieco\u00ebffici\u00ebnt-heatmaps te maken. In afbeelding 7: Big Top's boardroom correlaties is een voorbeeld van deze correlatieco\u00ebffici\u00ebnt-heatmaps te zien. Afbeelding 7: Big Top's Boardroom correlaties Hieruit kon opgemaakt worden dat er meerdere waarden zijn die redelijk sterk met elkaar gecorreleerd zijn. Bijvoorbeeld TVOC & temperatuur, dit zou een indicatie kunnen zijn dat wanneer de temperatuur stijgt de TVOC-waarde meestijgt. Om dit te kunnen uitsluiten zal per meetwaarden verder onderzoek verricht moeten worden. Missende gegevens In de onderstaande afbeelding is het percentage missende gevisualiseerd. Het is opvallend dat de missende data in clusters opgedeeld kan worden, namelijk: TVOC, Pressure, CO2, Illumunation, Activity TempInt, Humidity DOOR_OPEN_TIMES, DOOR_OPEN_STATUS Afbeelding 8: Big Top's missende gegevens Na de applicatie beter te bekijken is bevonden dat er per ruimte andere gegevens worden bijgehouden. Hierdoor is de clustervorming van de missende gegevens te verklaren. Uitschieters Tijdens de univariate analyse is gebleken dat sommige meetwaarden veel uitschieters bevatten. Om deze op te lossen is de onderstaande \"interquartile range outlier removal\" methode gebruikt. def iqr_outlier_removal(df, scale=1.5): \"\"\"Removes outliers based on the interquartile range method.\"\"\" # Calculate 1st and 3rd quantile and the range between them (iqr) q1 = df.quantile(0.25) q3 = df.quantile(0.75) iqr = q3 - q1 # Calculate the upper- and lower boundries lower = q1 - scale * iqr upper = q3 + scale * iqr # Filter out non numeric data non_numeric = df.select_dtypes(exclude=np.number).columns.values df_in = df.select_dtypes(np.number) # Filter dataframe based on upper- and lower boundries df_out = df_in[~((df_in < lower) | (df_in > upper)).any(axis=1)] print(f'Removed {df_in.shape[0] - df_out.shape[0]} outliers.') # Rejoin non numeric data and return resulting dataframe return df_out.join(df[non_numeric]) Wanneer deze methode wordt gebruikt op de datasets verwijderd het ongeveer 20% van de data. Het kan zijn dat dit teveel is en dat er niet genoeg data over is om effectieve modellen van te maken. In dit geval kan de scale parameter worden aangepast zodat er minder data als uitschieter wordt gezien. Bevindingen Deze Exploratory Data Analyse is uitgevoerd om te kijken hoe de data die door de Twindle-applicatie verzameld werd in elkaar zit. De volgende bevindingen zijn gemaakt: De gegevens bevatten grote uitschieters. Niet voor elke kamer worden dezelfde gegevens verzameld. De gegevens zijn erg scheef, wat onnauwkeurigheden kan veroorzaken bij gebruik in regressie-algoritmen. Er is een gebrek aan sterk gecorreleerde features. De methode voor het verwijderen van uitschieters verwijdert ongeveer ~20% van de gegevens; de scale parameter kan worden aangepast om dit te verlagen. Op basis van deze bevindingen kunnen de datarequirements worden onderzocht. Requirements vanuit de voorgaande analyses zijn requirements opgesteld. Deze vallen in de volgende drie categorieen: Software Data Machine learning Software requirements Gebaseerd op het resultaat van de story mappingsessie zijn de volgende requirements opgesteld voor het softwareproduct. Deze zijn geprioriteerd door middel van de MosCoW-methode 10 . ID Beschrijving Prioritijd 1. Tijdlijn waarop de toekomstige- en verleden meldingen te zien zijn. Must 2. Meldingen wanneer luchtkwaliteit te laag dreigt te worden. Must 3. Comments plaatsen op de tijdlijn. Must 4. Uitschieters rapportage. Could 5. Energiebesparing voorspellen om exessen te voorkomen. Could 6. Preventief onderhoud voorspellen*. Won't 7. Indicatie waar personen zich bevinden*. Won't De kern van het systeem zal zijn dat er meldingen gegeven kunnen worden wanneer de luchtkwaliteit in gevaar is. Hiervoor zal een tijdlijn aan de bestaande applicatie worden toegevoegd om deze meldingen zichtbaar te maken. *In het resultaat van de storymapping staat beschreven dat requirement #7 bij het proof of concept hoort. Later is besloten om de scope in te perken tot luchtkwaliteit voorspellingen. Data requirements Om te bepalen of aan de luchtkwaliteitseisen wordt voldaan is een overzicht samengesteld met minima en maxima opgesteld, zie tabel 1: minima & maxima waarden . Om deze waarden effectief te kunnen voorspellen zal extra data verzameld moeten worden, per meetwaarde is hier onderzoek naar verricht 11 . Meetwaarde Min (waarschuwing) Max (waarschuwing) Min (gevaar) Max (gevaar) Temperatuur 17 22,5 16,5 23,5 Luchtvochtigheid 40 60 30 70 CO2 400 800 300 1200 TVOC -1 200 -1 250 tabel 1: minima & maxima waarden Uit dit onderzoek is de volgende lijst met te verzamelen data gekomen: Gebouw Oppervlakte van de ruimten Bouwjaar Verwarming/airconditioning instellingen Raamstand Weer Buitentemperatuur Luchtvochtigheid Zonnestraling Machine learning requirements Vanuit de product owner is het doel gesteld dat de modellen tenminste 90% accuraat moeten zijn. De meetwaarden die gemodelleerd dienen te worden zijn continu en niet uit te drukken in procent accuracy. R-squared score kan gebruikt worden voor dit soort modellen. Het geeft een waarde tussen 0.0 en 1.0 aan wat ge\u00efnterpreteerd kan worden als een percentage. Wanneer een R2-score van 0.90 wordt behaald zal het model aan de eisen voldoen. Om de modellen onderling met elkaar te vergelijken zal, naast r-squared, ook de root-mean-squared-error-scoringmethode worden toegepast. Deze methode resulteert in een waarde in dezelfde eenheid als het target. Hierdoor kan gezien worden in hoeverre een gemiddelde voorspelling zal afwijken van de realiteit. Stakeholdersanalyse \u21a9 LucidChart: How to do a stakeholder analysis \u21a9 Stakeholdersanalyse hoofdstuk 3: Begrijpen blz. 7 - 9 \u21a9 AltexSoft: A Complete Guide to User Story Mapping \u21a9 Story mappingrapport \u21a9 Story mappingrapport hoofdstuk 1.1; Personas blz. 5 - 6 \u21a9 Exploratory Data Analyse \u21a9 Analytics Vidya: A Comprehensive Guide to Data Exploration \u21a9 Exploratory Data Analyse: 1.1 Insights \u21a9 MosCow-methode \u21a9 Data requirementsrapport \u21a9","title":"Analyseren"},{"location":"analyseren/#stakeholders-analyse","text":"Om te bepalen welke partijen relevant waren voor het project was een stakeholdersanalyse uitgevoerd 1 . De methodiek om de stakeholdersanalyse uit te voeren bestaat uit het identificeren- , prioriteren- en het begrijpen van deze groepen personen 2 .","title":"Stakeholders analyse"},{"location":"analyseren/#identificeren","text":"Om de stakeholders te kunnen identificeren was een brainstormsessie gehouden. Het resultaat van deze sessie is een mindmap van alle personen of entiteiten die invloed hebben op het project. In de onderstaande afbeelding is deze mindmap te zien. Afbeelding 1: Stakeholders-mindmap","title":"Identificeren"},{"location":"analyseren/#prioriteren","text":"Om de communicatiemethoden te bepalen zijn de stakeholders geprioriteerd. Dit was gedaan door te kijken wat hun niveau van invloed is en hoeveel belang zij hadden bij het project. In het onderstaande \u201cStakeholder Power Interest Grid\u201d is dit gevisualiseerd. Afbeelding 2: Power-interest grid","title":"Prioriteren"},{"location":"analyseren/#begrijpen","text":"De volgende stap was om, per stakeholder, te begrijpen wat hun relatie was tot het project. Dit was gedaan door per stakeholdercategorie een aantal vragen te stellen en te beantwoorden 3 . Gebaseerd op deze vragen is het onderstaande communicatieplan opgesteld. Afbeelding 3: Communicatie plan","title":"Begrijpen"},{"location":"analyseren/#story-mapping","text":"Gedurende het project was een agile werkwijze worden gehanteerd. Een van de onderdelen hiervan is een backlog waarin alle taken waren vastgelegd. De techniek die gebruikt was voor het opstellen van deze backlog is story mapping 4 . De voorbereiding en conclusie van deze story mappingsessie 5 worden onderstaand toegelicht.","title":"Story mapping"},{"location":"analyseren/#voorbereiding","text":"Aan de hand van het communicatieplan, zie afbeelding 3: Communicatie plan was een lijst met personen opgesteld die aanwezig moesten zijn bij de story mappingsessie. Afbeelding 4: Aanwezigen Personas waren toegepast om een duidelijker beeld te krijgen bij de gebruikers van de applicatie. Uiteindelijk was vastgesteld dat er twee gebruikersgroepen binnen de applicatie zijn 6 . Hotelmanagers & consumers.","title":"Voorbereiding"},{"location":"analyseren/#resultaat","text":"De story mapping sessie heeft plaatsgevonden op 02-03-2021. Om iedereen duidelijk te maken wat er verwacht werd was een korte presentatie gegeven. Daarna is de onderstaande storymap opgesteld. Deze is gebruikt om via Trello een backlog op te stellen. Afbeelding 5: Storymap","title":"Resultaat"},{"location":"analyseren/#exploratory-data-analyse","text":"De eerste stap om te kunnen bepalen welke machine learningmodellen toegepast konden worden was het analyseren van de data die Twindle tot nu toe verzameld heeft. Hiervoor is een Exploratory Data Analyse 7 (EDA) gemaakt volgens de methode van Sunil Ray 8 . Dit bestaat uit de volgende onderdelen: Variable identificatie Univariate analyse Multivariate analyse Missende gegevens Uitschieters","title":"Exploratory Data Analyse"},{"location":"analyseren/#variable-identificatie","text":"De volgende gegevens, en hun datatype, waren aanwezig in de dataset. Een overzicht van de handelingen die gedaan waren om de onjuiste datatypes op te lossen kan gevonden worden in hoofdstuk 1.1 van de EDA 9 . TVOC float64 # 'Total volatile organic componds' in PPB Pressure float64 # Luchtdruk in Pa. CO2 float64 # CO2 concentratie in PPM. Illumination float64 # Verlichtingsniveau in flux. Activity float64 # 'Person in Room' (PIR) TimeStamp datetime64[ns] # Datum en tijd van meting TempInt float64 # Temperatuur in graden celsius Humidity float64 # Luchtvochtigheidspercentage DOOR_OPEN_STATUS float64 # Boolean of de deur open of dicht is. DOOR_OPEN_TIMES float64 # Aantal keer dat de deur geopend is. room object # Ruimte waarin de meting is vericht.","title":"Variable identificatie"},{"location":"analyseren/#univariate-analyse","text":"Univariate analyse is de eenvoudigste vorm van data analyse. Tijdens deze analyse werd elke variabele afzonderlijk geanalyseerd. Dit werd gedaan door de gegevens in histogrammen en boxplots te visualiseren. Hierdoor konden eventuele afwijkingen of uitschieters gedetecteerd worden. Afbeelding 6: Big Top's Boardroom distributie In de bovenstaande afbeelding zijn de distributies van alle meetwaarden in de boardroom van de Big Top gevisualiseerd. Hierin valt te zien dat de meeste waarden een redelijk normaal gedistribueerd zijn. Sommigen zoals bijvoorbeeld de CO2-gehalte hebben afwijkingen en uitschieters. Tijdens de modelleringfase is hier rekening mee gehouden.","title":"Univariate analyse"},{"location":"analyseren/#multivariate-analyse","text":"Tijdens dit soort data analyse zijn de verbanden tussen twee variabelen geanalyseerd. Dit is gedaan correlatieco\u00ebffici\u00ebnt-heatmaps te maken. In afbeelding 7: Big Top's boardroom correlaties is een voorbeeld van deze correlatieco\u00ebffici\u00ebnt-heatmaps te zien. Afbeelding 7: Big Top's Boardroom correlaties Hieruit kon opgemaakt worden dat er meerdere waarden zijn die redelijk sterk met elkaar gecorreleerd zijn. Bijvoorbeeld TVOC & temperatuur, dit zou een indicatie kunnen zijn dat wanneer de temperatuur stijgt de TVOC-waarde meestijgt. Om dit te kunnen uitsluiten zal per meetwaarden verder onderzoek verricht moeten worden.","title":"Multivariate analyse"},{"location":"analyseren/#missende-gegevens","text":"In de onderstaande afbeelding is het percentage missende gevisualiseerd. Het is opvallend dat de missende data in clusters opgedeeld kan worden, namelijk: TVOC, Pressure, CO2, Illumunation, Activity TempInt, Humidity DOOR_OPEN_TIMES, DOOR_OPEN_STATUS Afbeelding 8: Big Top's missende gegevens Na de applicatie beter te bekijken is bevonden dat er per ruimte andere gegevens worden bijgehouden. Hierdoor is de clustervorming van de missende gegevens te verklaren.","title":"Missende gegevens"},{"location":"analyseren/#uitschieters","text":"Tijdens de univariate analyse is gebleken dat sommige meetwaarden veel uitschieters bevatten. Om deze op te lossen is de onderstaande \"interquartile range outlier removal\" methode gebruikt. def iqr_outlier_removal(df, scale=1.5): \"\"\"Removes outliers based on the interquartile range method.\"\"\" # Calculate 1st and 3rd quantile and the range between them (iqr) q1 = df.quantile(0.25) q3 = df.quantile(0.75) iqr = q3 - q1 # Calculate the upper- and lower boundries lower = q1 - scale * iqr upper = q3 + scale * iqr # Filter out non numeric data non_numeric = df.select_dtypes(exclude=np.number).columns.values df_in = df.select_dtypes(np.number) # Filter dataframe based on upper- and lower boundries df_out = df_in[~((df_in < lower) | (df_in > upper)).any(axis=1)] print(f'Removed {df_in.shape[0] - df_out.shape[0]} outliers.') # Rejoin non numeric data and return resulting dataframe return df_out.join(df[non_numeric]) Wanneer deze methode wordt gebruikt op de datasets verwijderd het ongeveer 20% van de data. Het kan zijn dat dit teveel is en dat er niet genoeg data over is om effectieve modellen van te maken. In dit geval kan de scale parameter worden aangepast zodat er minder data als uitschieter wordt gezien.","title":"Uitschieters"},{"location":"analyseren/#bevindingen","text":"Deze Exploratory Data Analyse is uitgevoerd om te kijken hoe de data die door de Twindle-applicatie verzameld werd in elkaar zit. De volgende bevindingen zijn gemaakt: De gegevens bevatten grote uitschieters. Niet voor elke kamer worden dezelfde gegevens verzameld. De gegevens zijn erg scheef, wat onnauwkeurigheden kan veroorzaken bij gebruik in regressie-algoritmen. Er is een gebrek aan sterk gecorreleerde features. De methode voor het verwijderen van uitschieters verwijdert ongeveer ~20% van de gegevens; de scale parameter kan worden aangepast om dit te verlagen. Op basis van deze bevindingen kunnen de datarequirements worden onderzocht.","title":"Bevindingen"},{"location":"analyseren/#requirements","text":"vanuit de voorgaande analyses zijn requirements opgesteld. Deze vallen in de volgende drie categorieen: Software Data Machine learning","title":"Requirements"},{"location":"analyseren/#software-requirements","text":"Gebaseerd op het resultaat van de story mappingsessie zijn de volgende requirements opgesteld voor het softwareproduct. Deze zijn geprioriteerd door middel van de MosCoW-methode 10 . ID Beschrijving Prioritijd 1. Tijdlijn waarop de toekomstige- en verleden meldingen te zien zijn. Must 2. Meldingen wanneer luchtkwaliteit te laag dreigt te worden. Must 3. Comments plaatsen op de tijdlijn. Must 4. Uitschieters rapportage. Could 5. Energiebesparing voorspellen om exessen te voorkomen. Could 6. Preventief onderhoud voorspellen*. Won't 7. Indicatie waar personen zich bevinden*. Won't De kern van het systeem zal zijn dat er meldingen gegeven kunnen worden wanneer de luchtkwaliteit in gevaar is. Hiervoor zal een tijdlijn aan de bestaande applicatie worden toegevoegd om deze meldingen zichtbaar te maken. *In het resultaat van de storymapping staat beschreven dat requirement #7 bij het proof of concept hoort. Later is besloten om de scope in te perken tot luchtkwaliteit voorspellingen.","title":"Software requirements"},{"location":"analyseren/#data-requirements","text":"Om te bepalen of aan de luchtkwaliteitseisen wordt voldaan is een overzicht samengesteld met minima en maxima opgesteld, zie tabel 1: minima & maxima waarden . Om deze waarden effectief te kunnen voorspellen zal extra data verzameld moeten worden, per meetwaarde is hier onderzoek naar verricht 11 . Meetwaarde Min (waarschuwing) Max (waarschuwing) Min (gevaar) Max (gevaar) Temperatuur 17 22,5 16,5 23,5 Luchtvochtigheid 40 60 30 70 CO2 400 800 300 1200 TVOC -1 200 -1 250 tabel 1: minima & maxima waarden Uit dit onderzoek is de volgende lijst met te verzamelen data gekomen: Gebouw Oppervlakte van de ruimten Bouwjaar Verwarming/airconditioning instellingen Raamstand Weer Buitentemperatuur Luchtvochtigheid Zonnestraling","title":"Data requirements"},{"location":"analyseren/#machine-learning-requirements","text":"Vanuit de product owner is het doel gesteld dat de modellen tenminste 90% accuraat moeten zijn. De meetwaarden die gemodelleerd dienen te worden zijn continu en niet uit te drukken in procent accuracy. R-squared score kan gebruikt worden voor dit soort modellen. Het geeft een waarde tussen 0.0 en 1.0 aan wat ge\u00efnterpreteerd kan worden als een percentage. Wanneer een R2-score van 0.90 wordt behaald zal het model aan de eisen voldoen. Om de modellen onderling met elkaar te vergelijken zal, naast r-squared, ook de root-mean-squared-error-scoringmethode worden toegepast. Deze methode resulteert in een waarde in dezelfde eenheid als het target. Hierdoor kan gezien worden in hoeverre een gemiddelde voorspelling zal afwijken van de realiteit. Stakeholdersanalyse \u21a9 LucidChart: How to do a stakeholder analysis \u21a9 Stakeholdersanalyse hoofdstuk 3: Begrijpen blz. 7 - 9 \u21a9 AltexSoft: A Complete Guide to User Story Mapping \u21a9 Story mappingrapport \u21a9 Story mappingrapport hoofdstuk 1.1; Personas blz. 5 - 6 \u21a9 Exploratory Data Analyse \u21a9 Analytics Vidya: A Comprehensive Guide to Data Exploration \u21a9 Exploratory Data Analyse: 1.1 Insights \u21a9 MosCow-methode \u21a9 Data requirementsrapport \u21a9","title":"Machine learning requirements"},{"location":"beheer/","text":"Versiebeheer Het versiebeheersysteem wat binnen Handpicked Labs wordt toegepast is BitBucket . Om dit systeem zo soepel mogelijk te laten verlopen is de onderstaande branching strategie toegepast. Afbeelding 1: Branchingstrategie Deze strategie is succesvol toegepast, zie afbeelding 2. Door deze strategie toe te passen is er voor gezorgd dat merge-conflicten tot een minimum bleven en het mogelijk was om continous integration en deployment toe te passen. Afbeelding 2: Branchingstrategie implementatie Automatische tests Tijdens het ontwikkelen van de machine learning API is zoveel mogelijk gewerkt op een test-driven manier. Dit betekend dat methoden getests werden terwijl ze ge\u00efmplementeerd worden. Op deze manier kon gevalideerd worden dat de code werkt na verwachtingen. Onderstaand valt het resultaat van deze tests te zien. test_create_no_invalid_parameters_entity_persisted (tests.test_base_service.TestBaseService) ... ok test_delete_entity_entity_exists_deleted (tests.test_base_service.TestBaseService) ... ok test_get_all_none_exist_returns_empty_list (tests.test_base_service.TestBaseService) ... ok test_get_all_one_record_returns_list_of_one (tests.test_base_service.TestBaseService) ... ok test_get_by_filter_entity_does_not_exist_returns_empty_list (tests.test_base_service.TestBaseService) ... ok test_get_by_filter_entity_exists_returns_entity (tests.test_base_service.TestBaseService) ... ok test_get_by_id_entity_does_not_exist_returns_none (tests.test_base_service.TestBaseService) ... ok test_get_by_id_entity_exists_returns_entity (tests.test_base_service.TestBaseService) ... ok test_update_entity_not_found_raises_typeerror (tests.test_base_service.TestBaseService) ... ok test_update_entity_prohibited_raises_typeerror (tests.test_base_service.TestBaseService) ... ok test_get_between_dates_all_correct_four_forecasts_returned (tests.test_forecast_service.TestForecastService) ... ok test_get_between_dates_none_found_returns_empty_list (tests.test_forecast_service.TestForecastService) ... ok test_get_between_dates_not_datetime_raises_typerror (tests.test_forecast_service.TestForecastService) ... ok test_filter_between_date_end_before_start_raise_error (tests.test_ingestion_service.TestDummyIngestionService) ... ok test_remove_outliers_correct_dicts_outliers_were_removed (tests.test_preparation_methods.TestPreparationMethods) ... ok test_remove_outliers_min_higher_than_max_raises_value_error (tests.test_preparation_methods.TestPreparationMethods) ... ok test_resample_index_not_of_type_DatetimeIndex_raises_IndexError (tests.test_preparation_methods.TestPreparationMethods) ... ok test_resample_interpolate_returns_resamples_dataframe (tests.test_preparation_methods.TestPreparationMethods) ... ok test_resample_time_unit_is_not_H_or_T_raises_ValueError (tests.test_preparation_methods.TestPreparationMethods) ... ok test_split_into_segments_datetimeindex_returns_tuple_of_lists (tests.test_preparation_methods.TestPreparationMethods) ... ok test_split_into_segments_no_datetimeindex_raises_typeerror (tests.test_preparation_methods.TestPreparationMethods) ... ok test_train_test_split_segments_correct_nr_of_rows_returnd (tests.test_preparation_methods.TestPreparationMethods) ... ok ---------------------------------------------------------------------- Ran 22 tests in 0.673s OK Deployment BitBucket biedt de mogelijkheid om via het BitBucket pipelines een automatische deployment straat in te richten. Momenteel is er bij Handpicked Labs geen omgeving waar deze pipelines de applicaties naar kunnen deployen. Om voorbereid te zijn op een toekomst wanneer dit wel mogelijk zou zijn is de onderstaande pipeline ontwikkeld. image: python:3.8 pipelines: default: - parallel: - step: name: Test caches: - pip script: - if [ -f requirements.txt ]; then pip install -r requirements.txt; fi - python -m unittest Feedback In de volgende secties valt de feedback op het project plan en portfolio, en hoe deze verwerkt is, te vinden. Project plan Sjoerd - 10-02-2021 In de introductie van het project plan stond dat er een Digital Twin ontwikkeld was voor het kantoorpand van Handpicked Agencies. Sjoerd had de opmerking dat er tevens een implementatie gemaakt is voor in hotelomgevingen. De introductie is herschreven zodat deze niet specifiek vermeld waar Digital Twins actief zijn. Daarnaast had Sjoerd nog feedback op de doelstelling. Ik had opgeschreven dat de Twindle applicatie uitgebreid kan worden zodat personen,bijvoorbeeld, de verwarming aan kunnen zetten zodra zij zien dat het te koud wordt. Volgens Sjoerd zou ik hier groter kunnen denken. Na aanleiding van deze feedback heb ik het doel vergroot naar grootschalige verbetering van de luchtkwaliteit waardoor de beleving van de personen in het gebouw vergroot wordt. Samet - 19-02-2021 Ik had beschreven dat we de applicatie wilde uitbreiden zodat deze informatie uit het verleden en toekomst kan tonen. Volgens Samet kon dit beter verwoord worden. Na aanleiding van deze feedback heb ik beter beschreven welke meetwaarden voorspeld gaan worden. Daarnaast had ik Samet een vraag gesteld waarom er gekozen was om TVOC waarde bij te meten en te gebruiken als luchtkwaliteit indicator. Zijn reactie hierop was dat voor luchtkwaliteit er meer nodig is dan alleen TVOC. Een combinatie van Co2, TVOC, Luchtvochtigheid en temperatuur zegt iets over de luchtkwaliteit. Tijdens de rest van het project kan ik er rekening mee houden dat deze waarden gebruikt worden voor luchtkwaliteit. Bartosz - 01-03-2021 \"Het is aan mij om te onderzoeken hoe Twindle het meest effectief ingezet kan worden voor deze implementatie\" het lijkt mij heel breed, kan je ket beter scopen? het gaat over softwre toch en niet business modelleren etc. je gaat een bestaande applicatie aanpassen zodat het model van een hotel van gemaakt kan worden? Wat is de baat van jouw project voor de handipicked en voor de hotel zelf? Het blijft nog steeds niet helder wat de kern van het probleem is dat je gaat oplosseen en wie erbij welke baat zal hebben (gecontrasteerd met de gebreken van de al bestaande oplossing). je deed een analyze in 1.2.1 op detail niveau, maar voeg die globale informatie wel toe in het begin van je document zoadt de lezer weet wat gaat dit project aanpakken. Introductie aangepast zodat hier een betere koppeling is tussen het probleem en de huidige implementatie. 1.3 gelimiteerde kennis van web hoort in risicos en niet hier. ook zijn de software aspecten van je project mogelijk hier al te vatten. je gaat backend en frontend werk doen, testen, data cleaning etc. Wat zijn de doelen dat je vanuit EDA wilt halen (hoe en waarvoor hoop je de restulaten ervan in te zetten in je daaropvolgende project delen?)? Begrenzing vastgesteld SW aspecten toegevoegd aan scope en randvoorwaarden Gelimiteerd web kennis verplaats naar risico's 1.4 als je naar die methodiek kijkt dan kan je hieruit ook een taal van deliverables nemen en expliciet noemen/verwerken in de rest van je document. bv. data requirements- ga je die opstellen of is dat niet in scope? deployments, hoe zie je dat voor zich? 1.5 \"zijn er ethische...\" vermijd ja nee vragen. better \"welke\" voeg ook iets toe over evaluatie van je oplossing, dat weet je nu nog niet zeker hoe dat het beste kan gebeuren.Weet je ook genoeg van air quality om te kunnen bepalen hoe en wat te meten en wat is goed en wat niet? 1.6 Tests niet vergeten, verder hoe worden je machine learning modellen getrained en ge\u00ebvalueerd is ook iets dat je gaat beschrijven misschien ga je er zelf een pipeline voor bouwen. Hoe ga je verschillende experimenten rapporteren en bijhouden? Model evaluatie beschreven Integratie verduidelijkt 3.2 Hoeveel interaties van het IBM process ga je uitvoeren? misschien helpt het bij het planing. probeer ook wat project specifieker te zijn in je planing (bv. welke onderzoeken ga je afronden in welke sprints? Producten en onderzoeken aan planning gekoppeld Verduidelijkt dat er twee iteraties van het IBM process uitgevoerd gaan worden. 4.1 Hoe ga je de integratie testen? En de software stukken? Automatisch, met hand? Test uitleg toegevoegd Over je vraag voor 4.3 Ik weet niet precies wat ik hier bij moet invullen. Zijn dit onderdelen zoals CI/CD en dergelijke? -> Ja, hoe je gaat omgaan met versie beheer maar ook deployments en bijhoordende configuraties. denk ook aan change management, wat en waar PR's en CR's, inhoud van je sprints hoe bijhouden? etc. 4.3 Versie beheer, deployment confgiguratie toegevoegd Bartosz 09-03-2021 Ik zou nog duidelijker de data preparation/cleaning opnemen in je PID. Scope uitgebreid met data verzameling en voorbereiding Scope randvoorwaarden verduidelijkt. Verder zie ik nog steeds \" Zijn er ethische aspecten\" ipv welke. heb je de laatste versie geupload? Deelvraag 4: \"Zijn er...\" naar \"Met welke...\" Voeg ook een deelvraag over air quality toe. Je hebt daar niet genoeg kennis van om te kunnen evalueren dat wat je doet goed is. Weet je ook genoeg van air quality om te kunnen bepalen hoe en wat te meten en wat goed is en wat niet? Deelvraag 3 specifiek op luchtkwaliteit gericht. Portfolio Bartosz op 19-05-2021 Evaluatie, zelf reflexie en conclusie bijvoegen in de leeswijzer. op hoog niveau van leeswijzer wil je laten zien wat het probleem was en in hoeverre het opgelost is (behaalde resultaten). Bartosz op 02-06-2021 Gebruik verleden tijd, omdat het een blik is op de al uitgevoerde werk/project. Project omgezet naar verleden tijd. De structuur dat je voorstelde met abstractie niveaus is goed. Je kan dan ook strategisch wat meer detail plaatsen in bv. leeswijzer -waar je zelf denkt dat je het meeste kan scoren. Want je wilt niet dat een opmerkelijke werk van je te onzichtbaar is. Je kan dus shinen door het beste werk stiekem expliciet naar voren te brengen in de leeswijzer. dat kan uiteraard niet te veel van zijn- dus wees selectief. planning zou ik weghalen van je introductie pagina. Te veel detail. Apart hoofdstuk voor planning gemaakt. \"Daarnaast heb ik de onderdelen van de leeswijzer een globale indruk gegeven van de inhoud.\" dat ga je straks nog met wat tekst verrijken neem ik aan? nu is als geplande inhoud fijn zo, later wil je hier al je werk gaan beschrijven. Zodra er een onderdeel van het portfolio af is vat ik dit samen in de leeswijzer. Dit eindigt in een kort conclusies & evaluaties en adviezen. in story_mapping.pdf zie ik niet vanuit de tekst een verwijzing naar de bronnen die je helemaal op het einde had bijgevoegd. Verwijs je ernaar? Dat zou moeten. Zo ver ik zie zijn alle bronnen op de APA stijl opgesteld en wordt er naar verwezen in de tekst. Bij het analyseren zou ik wat meer requirements willen zien. Dus wat was gevraagd om te bouwen uiteindelijk, en over ML aspecten vind ik ook maar amper iets. Apart hoofdstuk gemaakt voor: Software requirements Data requirements Machine learning requirements Onderhouden heet -> Beheer Onderdeel hernoemt. Bartosz op 10-06-2021 Ziet en heel goed eruit, toegankelijk en gelaagd portfolio. leest fijn en de content is interessant. Check even heel goed of je alle competenties dekt? ik mis bijvoorbeeld communicatie. Zelf reflectie en evaluatie is ook handig (op het einde van de leeswijzer) Communicatie toon ik aan door de presentaties die ik heb gegeven tijdens de opleveringen te beschrijven. Daarnaast is het portfolio zelf en de eindpresentatie hier ook onderdeel van. Evaluatie/zelfreflectie toegevoegd aan de conclusie op de leeswijzer. je moet nog paar pagina's vullen met tekst maar als je het doet zoals je deed tot nu toe dan komt alles goed. Ik vind het een fijn portfolio. Ik kwam nog iets tegen. Ga je geen code/notebooks opleveren met je portfolio? Je verhaal is fijn maar ik zou meer producten willen zien van je werk, dus concrete deliverables waaraan jij gewerkt hebt. Model experiment notebooks toegevoegd: Linear regressie Exponential smoothing ARIMA Short term Long term","title":"Beheer"},{"location":"beheer/#versiebeheer","text":"Het versiebeheersysteem wat binnen Handpicked Labs wordt toegepast is BitBucket . Om dit systeem zo soepel mogelijk te laten verlopen is de onderstaande branching strategie toegepast. Afbeelding 1: Branchingstrategie Deze strategie is succesvol toegepast, zie afbeelding 2. Door deze strategie toe te passen is er voor gezorgd dat merge-conflicten tot een minimum bleven en het mogelijk was om continous integration en deployment toe te passen. Afbeelding 2: Branchingstrategie implementatie","title":"Versiebeheer"},{"location":"beheer/#automatische-tests","text":"Tijdens het ontwikkelen van de machine learning API is zoveel mogelijk gewerkt op een test-driven manier. Dit betekend dat methoden getests werden terwijl ze ge\u00efmplementeerd worden. Op deze manier kon gevalideerd worden dat de code werkt na verwachtingen. Onderstaand valt het resultaat van deze tests te zien. test_create_no_invalid_parameters_entity_persisted (tests.test_base_service.TestBaseService) ... ok test_delete_entity_entity_exists_deleted (tests.test_base_service.TestBaseService) ... ok test_get_all_none_exist_returns_empty_list (tests.test_base_service.TestBaseService) ... ok test_get_all_one_record_returns_list_of_one (tests.test_base_service.TestBaseService) ... ok test_get_by_filter_entity_does_not_exist_returns_empty_list (tests.test_base_service.TestBaseService) ... ok test_get_by_filter_entity_exists_returns_entity (tests.test_base_service.TestBaseService) ... ok test_get_by_id_entity_does_not_exist_returns_none (tests.test_base_service.TestBaseService) ... ok test_get_by_id_entity_exists_returns_entity (tests.test_base_service.TestBaseService) ... ok test_update_entity_not_found_raises_typeerror (tests.test_base_service.TestBaseService) ... ok test_update_entity_prohibited_raises_typeerror (tests.test_base_service.TestBaseService) ... ok test_get_between_dates_all_correct_four_forecasts_returned (tests.test_forecast_service.TestForecastService) ... ok test_get_between_dates_none_found_returns_empty_list (tests.test_forecast_service.TestForecastService) ... ok test_get_between_dates_not_datetime_raises_typerror (tests.test_forecast_service.TestForecastService) ... ok test_filter_between_date_end_before_start_raise_error (tests.test_ingestion_service.TestDummyIngestionService) ... ok test_remove_outliers_correct_dicts_outliers_were_removed (tests.test_preparation_methods.TestPreparationMethods) ... ok test_remove_outliers_min_higher_than_max_raises_value_error (tests.test_preparation_methods.TestPreparationMethods) ... ok test_resample_index_not_of_type_DatetimeIndex_raises_IndexError (tests.test_preparation_methods.TestPreparationMethods) ... ok test_resample_interpolate_returns_resamples_dataframe (tests.test_preparation_methods.TestPreparationMethods) ... ok test_resample_time_unit_is_not_H_or_T_raises_ValueError (tests.test_preparation_methods.TestPreparationMethods) ... ok test_split_into_segments_datetimeindex_returns_tuple_of_lists (tests.test_preparation_methods.TestPreparationMethods) ... ok test_split_into_segments_no_datetimeindex_raises_typeerror (tests.test_preparation_methods.TestPreparationMethods) ... ok test_train_test_split_segments_correct_nr_of_rows_returnd (tests.test_preparation_methods.TestPreparationMethods) ... ok ---------------------------------------------------------------------- Ran 22 tests in 0.673s OK","title":"Automatische tests"},{"location":"beheer/#deployment","text":"BitBucket biedt de mogelijkheid om via het BitBucket pipelines een automatische deployment straat in te richten. Momenteel is er bij Handpicked Labs geen omgeving waar deze pipelines de applicaties naar kunnen deployen. Om voorbereid te zijn op een toekomst wanneer dit wel mogelijk zou zijn is de onderstaande pipeline ontwikkeld. image: python:3.8 pipelines: default: - parallel: - step: name: Test caches: - pip script: - if [ -f requirements.txt ]; then pip install -r requirements.txt; fi - python -m unittest","title":"Deployment"},{"location":"beheer/#feedback","text":"In de volgende secties valt de feedback op het project plan en portfolio, en hoe deze verwerkt is, te vinden.","title":"Feedback"},{"location":"beheer/#project-plan","text":"Sjoerd - 10-02-2021 In de introductie van het project plan stond dat er een Digital Twin ontwikkeld was voor het kantoorpand van Handpicked Agencies. Sjoerd had de opmerking dat er tevens een implementatie gemaakt is voor in hotelomgevingen. De introductie is herschreven zodat deze niet specifiek vermeld waar Digital Twins actief zijn. Daarnaast had Sjoerd nog feedback op de doelstelling. Ik had opgeschreven dat de Twindle applicatie uitgebreid kan worden zodat personen,bijvoorbeeld, de verwarming aan kunnen zetten zodra zij zien dat het te koud wordt. Volgens Sjoerd zou ik hier groter kunnen denken. Na aanleiding van deze feedback heb ik het doel vergroot naar grootschalige verbetering van de luchtkwaliteit waardoor de beleving van de personen in het gebouw vergroot wordt. Samet - 19-02-2021 Ik had beschreven dat we de applicatie wilde uitbreiden zodat deze informatie uit het verleden en toekomst kan tonen. Volgens Samet kon dit beter verwoord worden. Na aanleiding van deze feedback heb ik beter beschreven welke meetwaarden voorspeld gaan worden. Daarnaast had ik Samet een vraag gesteld waarom er gekozen was om TVOC waarde bij te meten en te gebruiken als luchtkwaliteit indicator. Zijn reactie hierop was dat voor luchtkwaliteit er meer nodig is dan alleen TVOC. Een combinatie van Co2, TVOC, Luchtvochtigheid en temperatuur zegt iets over de luchtkwaliteit. Tijdens de rest van het project kan ik er rekening mee houden dat deze waarden gebruikt worden voor luchtkwaliteit. Bartosz - 01-03-2021 \"Het is aan mij om te onderzoeken hoe Twindle het meest effectief ingezet kan worden voor deze implementatie\" het lijkt mij heel breed, kan je ket beter scopen? het gaat over softwre toch en niet business modelleren etc. je gaat een bestaande applicatie aanpassen zodat het model van een hotel van gemaakt kan worden? Wat is de baat van jouw project voor de handipicked en voor de hotel zelf? Het blijft nog steeds niet helder wat de kern van het probleem is dat je gaat oplosseen en wie erbij welke baat zal hebben (gecontrasteerd met de gebreken van de al bestaande oplossing). je deed een analyze in 1.2.1 op detail niveau, maar voeg die globale informatie wel toe in het begin van je document zoadt de lezer weet wat gaat dit project aanpakken. Introductie aangepast zodat hier een betere koppeling is tussen het probleem en de huidige implementatie. 1.3 gelimiteerde kennis van web hoort in risicos en niet hier. ook zijn de software aspecten van je project mogelijk hier al te vatten. je gaat backend en frontend werk doen, testen, data cleaning etc. Wat zijn de doelen dat je vanuit EDA wilt halen (hoe en waarvoor hoop je de restulaten ervan in te zetten in je daaropvolgende project delen?)? Begrenzing vastgesteld SW aspecten toegevoegd aan scope en randvoorwaarden Gelimiteerd web kennis verplaats naar risico's 1.4 als je naar die methodiek kijkt dan kan je hieruit ook een taal van deliverables nemen en expliciet noemen/verwerken in de rest van je document. bv. data requirements- ga je die opstellen of is dat niet in scope? deployments, hoe zie je dat voor zich? 1.5 \"zijn er ethische...\" vermijd ja nee vragen. better \"welke\" voeg ook iets toe over evaluatie van je oplossing, dat weet je nu nog niet zeker hoe dat het beste kan gebeuren.Weet je ook genoeg van air quality om te kunnen bepalen hoe en wat te meten en wat is goed en wat niet? 1.6 Tests niet vergeten, verder hoe worden je machine learning modellen getrained en ge\u00ebvalueerd is ook iets dat je gaat beschrijven misschien ga je er zelf een pipeline voor bouwen. Hoe ga je verschillende experimenten rapporteren en bijhouden? Model evaluatie beschreven Integratie verduidelijkt 3.2 Hoeveel interaties van het IBM process ga je uitvoeren? misschien helpt het bij het planing. probeer ook wat project specifieker te zijn in je planing (bv. welke onderzoeken ga je afronden in welke sprints? Producten en onderzoeken aan planning gekoppeld Verduidelijkt dat er twee iteraties van het IBM process uitgevoerd gaan worden. 4.1 Hoe ga je de integratie testen? En de software stukken? Automatisch, met hand? Test uitleg toegevoegd Over je vraag voor 4.3 Ik weet niet precies wat ik hier bij moet invullen. Zijn dit onderdelen zoals CI/CD en dergelijke? -> Ja, hoe je gaat omgaan met versie beheer maar ook deployments en bijhoordende configuraties. denk ook aan change management, wat en waar PR's en CR's, inhoud van je sprints hoe bijhouden? etc. 4.3 Versie beheer, deployment confgiguratie toegevoegd Bartosz 09-03-2021 Ik zou nog duidelijker de data preparation/cleaning opnemen in je PID. Scope uitgebreid met data verzameling en voorbereiding Scope randvoorwaarden verduidelijkt. Verder zie ik nog steeds \" Zijn er ethische aspecten\" ipv welke. heb je de laatste versie geupload? Deelvraag 4: \"Zijn er...\" naar \"Met welke...\" Voeg ook een deelvraag over air quality toe. Je hebt daar niet genoeg kennis van om te kunnen evalueren dat wat je doet goed is. Weet je ook genoeg van air quality om te kunnen bepalen hoe en wat te meten en wat goed is en wat niet? Deelvraag 3 specifiek op luchtkwaliteit gericht.","title":"Project plan"},{"location":"beheer/#portfolio","text":"Bartosz op 19-05-2021 Evaluatie, zelf reflexie en conclusie bijvoegen in de leeswijzer. op hoog niveau van leeswijzer wil je laten zien wat het probleem was en in hoeverre het opgelost is (behaalde resultaten). Bartosz op 02-06-2021 Gebruik verleden tijd, omdat het een blik is op de al uitgevoerde werk/project. Project omgezet naar verleden tijd. De structuur dat je voorstelde met abstractie niveaus is goed. Je kan dan ook strategisch wat meer detail plaatsen in bv. leeswijzer -waar je zelf denkt dat je het meeste kan scoren. Want je wilt niet dat een opmerkelijke werk van je te onzichtbaar is. Je kan dus shinen door het beste werk stiekem expliciet naar voren te brengen in de leeswijzer. dat kan uiteraard niet te veel van zijn- dus wees selectief. planning zou ik weghalen van je introductie pagina. Te veel detail. Apart hoofdstuk voor planning gemaakt. \"Daarnaast heb ik de onderdelen van de leeswijzer een globale indruk gegeven van de inhoud.\" dat ga je straks nog met wat tekst verrijken neem ik aan? nu is als geplande inhoud fijn zo, later wil je hier al je werk gaan beschrijven. Zodra er een onderdeel van het portfolio af is vat ik dit samen in de leeswijzer. Dit eindigt in een kort conclusies & evaluaties en adviezen. in story_mapping.pdf zie ik niet vanuit de tekst een verwijzing naar de bronnen die je helemaal op het einde had bijgevoegd. Verwijs je ernaar? Dat zou moeten. Zo ver ik zie zijn alle bronnen op de APA stijl opgesteld en wordt er naar verwezen in de tekst. Bij het analyseren zou ik wat meer requirements willen zien. Dus wat was gevraagd om te bouwen uiteindelijk, en over ML aspecten vind ik ook maar amper iets. Apart hoofdstuk gemaakt voor: Software requirements Data requirements Machine learning requirements Onderhouden heet -> Beheer Onderdeel hernoemt. Bartosz op 10-06-2021 Ziet en heel goed eruit, toegankelijk en gelaagd portfolio. leest fijn en de content is interessant. Check even heel goed of je alle competenties dekt? ik mis bijvoorbeeld communicatie. Zelf reflectie en evaluatie is ook handig (op het einde van de leeswijzer) Communicatie toon ik aan door de presentaties die ik heb gegeven tijdens de opleveringen te beschrijven. Daarnaast is het portfolio zelf en de eindpresentatie hier ook onderdeel van. Evaluatie/zelfreflectie toegevoegd aan de conclusie op de leeswijzer. je moet nog paar pagina's vullen met tekst maar als je het doet zoals je deed tot nu toe dan komt alles goed. Ik vind het een fijn portfolio. Ik kwam nog iets tegen. Ga je geen code/notebooks opleveren met je portfolio? Je verhaal is fijn maar ik zou meer producten willen zien van je werk, dus concrete deliverables waaraan jij gewerkt hebt. Model experiment notebooks toegevoegd: Linear regressie Exponential smoothing ARIMA Short term Long term","title":"Portfolio"},{"location":"blog/","text":"Welkom bij de blog Op deze pagina zal per week worden besproken welke taken er op de planning stonden, hoe deze zijn aangepakt, eventuele uitdagingen en een planning voor de volgende week. Week 1 08-02-2021 t/m 12-02-2021 Deze week ben ik gestart met mijn stageproject. Dit houdt in dat ik vooral bezig ben geweest met het projectplan. Daarnaast heb ik deze week ook toegang gekregen tot een kopie van de database. Projectplan Deze week was het doel om de kern het projectplan te beschrijven. Dit betekent dat ik veel aandacht heb besteed aan het opstellen van een doel & hoofd- en deelvragen. Momenteel luidt de hoofdvraag: Hoe kan machine learning waarde toevoegen voor Twindle-gebruikers? Om te helpen met het bepalen van het hoofddoel heb ik van Sjoerd een concept afbeelding ontvangen. Hierin wordt het toekomstbeeld van de Twindle weergegeven. Op deze afbeelding is een tijdlijn te zien waarin toekomstige waarschuwingen worden gevisualiseerd. Volgende week ga ik, door middel van een stakeholders analyse, uitzoeken wie de gebruikers zijn en wat zij graag aan de Twindle toegevoegd zien worden. Aan de hand van deze analyse kan ik het doel specifieker maken. Database Om een vlotte start te kunnen maken moet ik zo snel mogelijk toegang krijgen tot de database. Gelukkig kreeg ik dinsdag al een kopie van de database. Er was een kleine uitdaging met deze database, het is een document-based database. Aangezien ik data analyseer in Python notebooks door gebruik te maken van, onder andere, Pandas DataFrames moest ik hier een adapter voor schrijven. Dit bleek een veel voorkomend probleem en was binnen een uur opgelost. Volgende week of de week daarna verwacht ik aan de Exploratory Data Analyse te beginnen. De resultaten van deze analyse zullen gebruikt worden om te kijken of de gestelde doelen realistisch zijn. Week 2 15-02-2021 t/m 19-02-2021 Zoals vorige week afgesproken was heb ik deze week gewerkt aan een stakeholders analyse en het project plan. De volgende vooruitgang is hierin gemaakt. Stakeholders analyse De bedoeling was om een stakeholders analyse uit te voeren om de relevante partijen voor het project in kaart te brengen. Gebaseerd hierop kon een communicatieplan opgesteld worden. Dit is goed gelukt. In eerste instantie had ik had management van Postillion hotels in de hoogste categorie geplaatst. Na feedback van Sjoerd heb ik deze verwisseld met de personen van Techtenna. Volgende week ga ik deze personen uitnodigen voor een story mapping sessie en deze sessie voorbereiden. Project plan Deze week wilde ik de doelstelling verduidelijken. Dit heb ik gedaan door de SMART-methode toe te passen. Hieruit kwamen twee doelstellingen; Een die gericht was op energiebesparing en een andere voor luchtkwaliteit. Uiteindelijk heb ik na feedback van Samet ervoor gekozen om de focus te leggen op het luchtkwaliteit gedeelte. De eerste versie van het project plan is nu af. Ik heb het opgestuurd naar mijn docent Bartosz voor feedback. Ik verwacht in de loop van volgende week deze feedback te ontvangen en toe te passen. Week 3 22-02-2021 t/m 26-02-2021 Deze week stond in het teken van de story mapping sessie en exploratory data analysis. Story mapping Aan de hand van de stakeholders analyse heb ik een voorbereiding gemaakt voor de story mapping ingepland. Deze zal volgende week dinsdag, twee maart, plaatsvinden. Hiervoor heb ik de digitale omgeving die tijdens de sessie gebruikt gaat worden alvast ingericht. Het resultaat van de story mapping zal een backlog zijn. Gebaseerd op deze backlog kan ik de verdere sprints inplannen. Exploratory Data Analysis Vorige week was ik begonnen met het ontwikkelen van een adapter om de database uit te lezen. Deze was nog niet goed genoeg. Ik heb het uitgebreid met een pipeline die de data opdeelt in de evenementen waarin ze opgeslagen worden een aan de hand van de timestamps aan elkaar verbonden. Dit resulteert in een net dataframe met minimale NaN-waarden. Hierna was ik begonnen aan de exploratory data analyse. Hieruit kwamen de volgende inzichten. Veel outliers. Correlatie verschil tussen ruimten. Gemeten data verschilt per ruimte. Volgende week wil ik eventuele oplossingen voor deze uitdagingen bespreken met met mijn technisch begeleider. Week 4 01-03-2021 t/m 05-03-2021 Deze week is er een nieuwe sprint begonnen. Het doel van deze sprint is om de data requirements op te stellen, de data te verzamelen en voor te bereiden om machine learning modellen te trainen. Deze week is aan de volgende onderdelen gewerkt; Story mapping, data requirements onderzoek en projectplan Story mapping Vorige week was ik begonnen om de story mapping sessie voor te bereiden. Deze week was er een afspraak gepland om deze sessie te houden. Samen met Jeroen & Marco van Techtenna hebben we besproken wat de richting van het project gaat worden. De bedoeling was dat hier een complete backlog uit kwam. Momenteel zijn het alleen nog \"epics\". Volgende week maandag wil ik bespreken welke concrete taken hierbij horen. Data requirements onderzoek Aan de hand van de exploratory data analyse heb ik een data requirements analyse uitgevoerd. Hierbij heb ik gekeken of verschillende modellen die moeten gaan worden ontwikkeld worden al door andere zijn ontwikkeld. Hiervoor heb ik een viertal raporrten gevonden waarop ik een \"data-boodschappenlijst\" heb kunnen samenstellen. De komende week hoop ik alle items van dit lijstje af te kunnen strepen. Projectplan Op maandag had ik feedback ontvangen over mijn projectplan van Bartosz. Dit had ik opgedeeld in onderdelen zodat ik op donderdag of vrijdag alle onderdelen verwerkt zou hebben. Ik heb mezelf aan deze planning kunnen houden en de tweede versie van het plan is nu af. Deze ga ik op sturen voor feedback of goedkeuring. Volgende week verwacht ik dit afgerond te hebben. Week 5 08-03-2021 t/m 12-03-2021 Deze week ben ik begonnen met het verzamelen van data uit externe bronnen. Daarnaast heb ik het projectplan afgerond. Dataverzameling Vorige week was ik ge\u00ebindigd met het samenstellen van een \"data-boodschappenlijstje\". Het doel was alle items deze week te verzamelen. In de meeste gevallen is dit gelukt. Zo heb ik bijvoorbeeld een bouwtekening van het kantoor ontvangen waaruit ik de oppervlakte van de ruimten heb gehaald. Er is veel meteorologische data benodigd. De grootste bron hiervan is het KNMI. Zij stellen een API beschikbaar waar accurate data over constante tijdsintervallen opgehaald kunnen worden. Het probleem is dat er momenteel technische problemen zijn met deze API waardoor hij niet beschikbaar is. Een oplossing die ik hier voor gevonden heb ik om historische data te downloaden en koppelen aan de data de vanuit Twindle verzameld wordt. Voor nu werkt dit en kan ik verder met het ontwikkelen van de machine learning modellen. Machine learning modellen Aangezien de data verzameling voorspoediger verliep dan verwacht kon ik een week eerder aan de machine learning model ontwikkeling beginnen. Ik heb er voor gekozen om te beginnen met relatief simpele lineaire regressie modellen. Hiermee heb ik al accurate resultaten kunnen bereiken zoals in de onderstaande afbeelding te zien valt. Volgende week wil ik voor de andere meetwaarden dezelfde techniek toepassen en kijken of het net zo goed werkt. Projectplan Deze week was de deadline voor het projectplan. Bartosz had nog wat kleine aanmerkingen. Deze heb ik verwerkt en zal ik volgende week of de week daarop bespreken. Week 6 15-03-2021 t/m 19-03-2021 Deze week was er een nieuwe sprint begonnen. Het doel van deze sprint is om een eerste prototype van de modellen op te leveren. Linear Regression Vorige week ben ik begonnen met het ontwikkelen van linear regression modellen. De eerste indruk was dat deze goed presteerde. Ik was wat achterdochtig over de 0.997 R2 score die de modellen behaalde en heb dat verder onderzocht. Het bleek dat de data seizoensgebonden en autocorrelaties bevat. Dit maakt traditionele regressie modellen ongeschikt. Zoals in de bovenstaande afbeelding resulteerd dit soort data in modellen die altijd de waarden van t-1 weergeven. In R2 scores en dergelijke lijkt het te werken maar uiteindelijk bevat het model geen voorspellende capaciteiten. Time Series Prediction Na wat onderzoek te hebben gedaan ben ik uit gekomen op time series prediction. Dit is iets waar ik nog nooit eerder mee heb gewerkt en wat aardig complex is. Momenteel heb ik de data verder onderzocht op seizoensgebondenheid, autocorrelatie en trends. Vanuit hier wil ik volgende week met technieken zoals ARIMA aan de slag om modellen te ontwikkelen. Week 7 22-03-2021 t/m 26-03-2021 vorige week was ik tot de conclusie gekomen dat mijn lineare regressie modellen geen voorspellende waarde hadden. Deze week heb ik verder onderzoek gedaan naar time series prediction en de volgende drie model soorten uitgeprobeerd; Linear regression, Exponential Smoothing & ARIMA. Voor ik begin met de experimenten had ik vastgesteld dat modellen beoordeeld worden op R2 score. Een model moet een minimale R2 score van 0.90 halen om als goed beschouwd te worden. Linear Regression Omdat de data erg sterke autocorrelaties bevat heb ik de keuze gemaakt om in plaats van de echte waarde te voorspellen het verschil tussen twee punten te voorspellen. De modellen die ik hier mee ontwikkeld heb komen niet boven de 0.30 R2 score uit en zijn dus niet goed genoeg. Waarschijnlijk mis ik nog belangrijke features die leiden tot een verandering in de meetwaarde. Exponential Smoothing & ARIMA Deze technieken werkte beide redelijk goed maar haalde het gestelde doel niet. Cross-validation moet toegepast worden om te bepalen hoe goed het model echt presteert. Volgende week wil ik beginnen met het ontwikkelen van de integratie van de modellen in Twindle. Hiervoor zal ik beginnen met een ontwerp maken wat ik daarna bespreek met mijn technisch begeleider. Parallel hieraan wil ik de modellen verder verbeteren. De eerste stap hierin is om meer data te verzamelen die invloed kan hebben op veranderingen in de meetwaarde. Week 8 29-03-2021 t/m 02-04-2021 Deze week was er een nieuwe sprint begonnen. Deze sprint stond in het teken van het ontwerpen en ontwikkelen van de integratie van de machine learning modellen in de bestaande Twindle applicatie. Ontwerp De koppeling van de machine learning modellen is ontworpen volgend het C4 model. Op deze manier kan van een hoog tot laag abstractie niveau gewerkt worden. Het belangrijkste aan dit ontwerp is dat de machine learning modellen perdiodiek ge\u00fcpdatet moeten worden met nieuwe data. Hiervoor is goed nagedacht over de communicatie met de API van de Twindle applicatie. De eerste versie van het ontwerp is afgerond. Volgende week zal ik met mijn technisch begeleider dit ontwerp bespreken, aanpassingen maken en beginnen met de implementatie. Planning Het plan was om deze sprint de applicatie te ontwerpen en te implementeren. Waarschijnlijk is deze planning te optimistisch. Zoals het er nu voor staat zal er volgende week donderdag begonnen worden aan de implementatie. Een extra sprint zal benodigd zijn om deze af te ronden. Tijdens het opstellen van het project plan is rekening gehouden dat deze situatie zich voor kon doen. Week 9 05-04-2021 t/m 02-04-2021 Vorige week was vastgesteld dat er een extra sprint benodigd was om de applicatie te kunnen implementeren. Deze week is verder gewerkt aan het ontwerp. Daarnaast is er een oplevering geweest voor Techtenna waar een verandering van het doel is besproken. Ontwerp Vorige week was een groot gedeelte van het ontwerp afgerond. Deze week is dit met Samet besproken. Er moest nog meer verduidelijkt worden over de database, het was nog niet duidelijk wat voor soort database toegepast ging worden en met welke technieken. Hier is verder onderzoek naar gedaan waarvan de conclusie was dat er een MySQL database gebruikt gaat worden. Momenteel is er een andere stagiaire, Dmitri, ook bezig aan uitbreidingen van de Twindle applicatie. Samen met hem is het ontwerp besproken om ervoor te zorgen dat we elkaar niet tegenwerken. Oplevering & doelwijziging Na aanleiding van de storymapping sessie in week vier was er besloten iedere maand een update te geven aan de personen van Techtenna. Hiervoor was er een presentatie voorbereid waarin de bevindingen van de machine learning experimenten en het ontwerp werden besproken. Tijdens het ontwerpen van de applicatie stuiten ik op meerdere artikelen waarin verteld werd dat een groot gedeelte van de machine learning projecten nooit afgerond word. Dit komt doordat het proces wat gebruikt wordt om machine learning modellen te ontwikkelen niet per se goed aansluit bij software- en DevOps processen. Hierdoor had ik voorgesteld om in plaats van voor de drie service lagen modellen te ontwikkelen de focus te leggen op de luchtkwaliteit modellen. Zo kan ik een systeem ontwikkelen wat ervoor zal zorgen dat de machine learning modellen continu worden ge\u00fcpdatet met de nieuwste data en bijgestuurd kunnen worden waar nodig. Dit voorstel werd positief ontvangen door mijn stagebegeleiders en Marco van Techtenna. Het ontwerp is nu afgerond en goedgekeurd en zal de komende twee weken worden geimplementeerd. Week 10 12-04-2021 t/m 16-04-2021 Vorige week was het ontwerp van mijn applicatie goedgekeurd. Het bestaat uit drie onderdelen, ML API, Twindle front-end en Twindle back-end. Deze week is gewerkt aan het eerste onderdeel, de ML API. Pipeline Het proces wat nu in de notebooks gebruikt wordt om de ARIMA modellen te maken moest worden omgezet naar een geautomatiseerde pipeline. Hiervoor heb ik een modules gemaakt die data kan opvragen uit het Twindle back-end, deze data kunnen voorbereiden en de modellen kunnen updaten en daarna opslaan in een database. Hierbij liep ik tegen het probleem aan dat de .pkl bestanden ~1.5 GB per stuk waren. Dit bleek te groot te zijn om in een SQL database op te slaan. Na wat onderzoek te doen en mogelijkheden te bespreken heb ik besloten om het pad naar de bestanden op te slaan i.p.v. de complete modellen. API De forecasts die gemaakt worden na het updaten van de modellen moeten kunnen worden opgevraagd via een API. Hiervoor heb ik de Flask library toegepast. Ik verwachte dat dit een redelijk simpel proces was aangezien ik maar 1 endpoint hoefde te ontwikkelen. Aangezien ik niet veel ervaring heb met het ontwikkelen van applicaties in Python ging dit wat minder soepel dan verwacht. Uiteindelijk is het me gelukt om dit endpoint toe te voegen. De manier waarop ben ik alleen nog niet helemaal tevreden over. Volgende week maandag heb ik een technische bespreking met Samet waarin ik, onder andere, dit wil aankaarten. Week 11 19-04-2021 t/m 23-04-2021 Deze week heb ik samen met mijn technisch begeleider naar de ML pipeline en API gekeken. Hieruit kwam de volgende feedback: Pipeline meer SOLID opstellen Mappen structuur voor file opslag verbeteren Zorgen dat de code niet te lang wordt. Eerder onderbreken. Daarnaast vondt hij het goed dat ik veel comments heb geplaatst, gebruik maak van docstrings en een groot gedeelte van de applicatie automatisch getest kan worden via unit tests. Afonronding Pipeline Nadat de feedback verwerkt was ben ik verder gegaan met het ontwikkelen van de pipeline. Voornamelijk heb ik aan het systeem gewerkt waardoor er steeds een tijdsinterval naar voor wordt verwerkt. Dit looped totdat er geen data meer beschikbaar is of tot dat het tijdstip waarnaar ge\u00fcpdatet dient te worden in de toekomst valt. Dit syteem werkt goed, ik wel gemerkt dat de voorspellingen altijd twee uur achterlopen. Waarschijnlijk komt dit doordat ik een fout heb gemaakt tijdens het berekenen van de datum. Volgende week ga ik dit probleem uitzoeken. Week 12 28-04-2021 t/m 30-04-2021 Deze week heb ik een aantal dagen vrij genomen in verband met Koningsdag. Daardoor heb ik deze week maar drie dagen gewerkt. Deze week stond in het teken van de laatste bugs uit de ML pipeline halen. Bugs De volgende bugs zijn gevonden en opgelost. ARIMA Model werd geladen, ge\u00fcpdatet maar niet opgeslagen. Opgelost door model iedere iteratie op te slaan. Datum berekening liep een uur voor. Na deze veranderingen door te voeren heb ik het model ge\u00ebvalueerd. Hieruit blijkt dat de voorspellingen nog steeds een uur achter lopen. Het kan zijn dat dit komt door het model zelf en niet een fout in de pipeline. Als dit zo is zou dat betekenen dat dit model waarschijnlijk niet gaat werken voor dit systeem. Portfolio Tijdens de terugkomdag werd ons als tip meegegeven om iedere week een dag te besteden aan het portfolio. Naar aanleiding van deze feedback heb ik besloten om iedere vrijdag aan het portfolio te gaan werken. Deze week heb ik uitgezocht hoe ik MkDocs zo kan toepassen dat ik een online versie kan hebben en een versie die ik als bestanden kan inleveren. Daarnaast ben ik begonnen aan het schrijven van de leeswijzer. Week 13 03-05-2021 t/m 07-05-2021 Deze week staat er op de planning om de eerste iteratie van de implementatie af te ronden. Deze week heb ik een aantal bugs opgelost uit de pipeline en het front-end uitgebreid met de voorspellingen die vanuit de pipeline worden gemaakt. Daarnaast heb ik deze week een oplevering gehad voor Techtenna. Pipeline Vorige week was ik begonnen met het het opslaan van de modellen na iedere iteratie. Hierdoor groeide de modellen erg snel in bestandsgrootte, 30GB +. Dit betekent dat de ARIMA modellen uit het pmdarima package niet bruikbaar zijn. Dit heb ik opgelost door ARIMA modellen uit het statsmodel package te gebruiken. Deze worden niet zo groot (~300 MB) en houden bij welke timestamps er gebruikt zijn. Zo kan ik beter bepalen met welke data het model ge\u00fcpdatet moet worden. Helaas blijft het probleem met de achterlopende voorspellingen bestaan. Hier ga ik verder op in tijdens de volgende onderwerpen. Front-end Ondanks dat de voorspellingen nog niet helemaal klopte wilde ik graag de front-end implementatie ontwikkelen. Dit kan ik namelijk meenemen tijdens de evaluatie van de modellen. Doordat ik de ML api goed heb getest ging de implementatie erg vlot. Oplevering Op woensdag heb ik een oplevering gehad voor Techtenna. Hieruit bleek dat de modellen waarschijnlijk niet goed gaan werken voor het probleem wat we proberen op te lossen. Marco adviseerde mij om feedback te vragen van een machine learning expert. Daarnaast verwacht hij dat we betere resultaten kunnen bereiken wanneer we meer data verzamelen die invloed hebben op de targets, bijvoorbeeld het aantal personen in een ruimte wat invloed heeft op de CO2 & TVOC waarde. Volgende maandag ga ik dit verder bespreken met mijn begeleiders. Daarnaast ga ik een afspraak inplannen met Bartosz om dit te bespreken. Week 14 10-05-2021 14-05-2021 Na aanleiding van de oplevering vorige week heb ik met Samet besloten om eerst een model te ontwikkelen voor de CO2-waarde. Op deze manier heb ik genoeg tijd om een model te ontwikkelen, de pipeline aan te passen en de front-end implementatie af te ronden. Data Requirements Marco van Techtenna vertelde tijdens de oplevering vorige week dat het aantal personen in een ruimte veel invloed heeft op de CO2 gehalte. Na verder onderzoek hierna te doen kon ik dit bevestigen. Om deze data te verzamelen heb ik besloten om de Google Calendar API te gebruiken. Via deze API kan ik gemakkelijk opvragen hoeveel personen er aanwezig zijn bij een geplande afpsraak. Korte termijn model De eerste stap die ik heb genomen om het korte termijn model te ontwikkelen was bekijken of andere personen dit probleem al hebben opgelost. Hieruit kwam een erg goed onderzoek met een duidelijke uitleg, code en dataset. De stappen die hierin genomen worden heb ik nagemaakt met de Twindle dataset. Hieruit is een model gekomen wat erg accuraat kan voorspellen wat de CO2-waarde over twee minuten zal zijn. Volgende week wil ik een model ontwikkelen wat voor de lange termijn een globaal overzicht kan geven. Deze twee modellen kunnen dat gekoppeld worden in het front-end om een compleet overzicht te geven. Week 15 17-05-2021 - 21-05-2021 Deze week is de planning om de tweede versie van de modellen af te ronden. Hiervoor moet nog een lange termijn model worden ontwikkelt. Lange termijn model In het onderzoek wat ik had gevonden voor het korte termijn model werdt gerefereerd naar een ander onderzoek wat op uurlijkse basis CO2 probeerde te voorspellen. Dit onderzoek heb ik doorgenomen. Helaas was het niet zo gedetailleerd als het andere onderzoek. Hierdoor heb ik zelf nog wat aanpassingen moeten maken aan het model. Uiteindelijk heb ik erg veel geprobeerd maar ben ik niet tot een juist model gekomen. Ik heb besloten om deze toch toe te passen, waarschijnlijk zou ik nog een complete stage periode bezig kunnen zijn met model experimenten uitvoeren. Ik vind het belangrijker dat ik uiteindelijk kan demonsteren dat het probleem opgelost wordt door mijn applicatie.","title":"Blog"},{"location":"blog/#welkom-bij-de-blog","text":"Op deze pagina zal per week worden besproken welke taken er op de planning stonden, hoe deze zijn aangepakt, eventuele uitdagingen en een planning voor de volgende week.","title":"Welkom bij de blog"},{"location":"blog/#week-1","text":"08-02-2021 t/m 12-02-2021 Deze week ben ik gestart met mijn stageproject. Dit houdt in dat ik vooral bezig ben geweest met het projectplan. Daarnaast heb ik deze week ook toegang gekregen tot een kopie van de database. Projectplan Deze week was het doel om de kern het projectplan te beschrijven. Dit betekent dat ik veel aandacht heb besteed aan het opstellen van een doel & hoofd- en deelvragen. Momenteel luidt de hoofdvraag: Hoe kan machine learning waarde toevoegen voor Twindle-gebruikers? Om te helpen met het bepalen van het hoofddoel heb ik van Sjoerd een concept afbeelding ontvangen. Hierin wordt het toekomstbeeld van de Twindle weergegeven. Op deze afbeelding is een tijdlijn te zien waarin toekomstige waarschuwingen worden gevisualiseerd. Volgende week ga ik, door middel van een stakeholders analyse, uitzoeken wie de gebruikers zijn en wat zij graag aan de Twindle toegevoegd zien worden. Aan de hand van deze analyse kan ik het doel specifieker maken. Database Om een vlotte start te kunnen maken moet ik zo snel mogelijk toegang krijgen tot de database. Gelukkig kreeg ik dinsdag al een kopie van de database. Er was een kleine uitdaging met deze database, het is een document-based database. Aangezien ik data analyseer in Python notebooks door gebruik te maken van, onder andere, Pandas DataFrames moest ik hier een adapter voor schrijven. Dit bleek een veel voorkomend probleem en was binnen een uur opgelost. Volgende week of de week daarna verwacht ik aan de Exploratory Data Analyse te beginnen. De resultaten van deze analyse zullen gebruikt worden om te kijken of de gestelde doelen realistisch zijn.","title":"Week 1"},{"location":"blog/#week-2","text":"15-02-2021 t/m 19-02-2021 Zoals vorige week afgesproken was heb ik deze week gewerkt aan een stakeholders analyse en het project plan. De volgende vooruitgang is hierin gemaakt. Stakeholders analyse De bedoeling was om een stakeholders analyse uit te voeren om de relevante partijen voor het project in kaart te brengen. Gebaseerd hierop kon een communicatieplan opgesteld worden. Dit is goed gelukt. In eerste instantie had ik had management van Postillion hotels in de hoogste categorie geplaatst. Na feedback van Sjoerd heb ik deze verwisseld met de personen van Techtenna. Volgende week ga ik deze personen uitnodigen voor een story mapping sessie en deze sessie voorbereiden. Project plan Deze week wilde ik de doelstelling verduidelijken. Dit heb ik gedaan door de SMART-methode toe te passen. Hieruit kwamen twee doelstellingen; Een die gericht was op energiebesparing en een andere voor luchtkwaliteit. Uiteindelijk heb ik na feedback van Samet ervoor gekozen om de focus te leggen op het luchtkwaliteit gedeelte. De eerste versie van het project plan is nu af. Ik heb het opgestuurd naar mijn docent Bartosz voor feedback. Ik verwacht in de loop van volgende week deze feedback te ontvangen en toe te passen.","title":"Week 2"},{"location":"blog/#week-3","text":"22-02-2021 t/m 26-02-2021 Deze week stond in het teken van de story mapping sessie en exploratory data analysis. Story mapping Aan de hand van de stakeholders analyse heb ik een voorbereiding gemaakt voor de story mapping ingepland. Deze zal volgende week dinsdag, twee maart, plaatsvinden. Hiervoor heb ik de digitale omgeving die tijdens de sessie gebruikt gaat worden alvast ingericht. Het resultaat van de story mapping zal een backlog zijn. Gebaseerd op deze backlog kan ik de verdere sprints inplannen. Exploratory Data Analysis Vorige week was ik begonnen met het ontwikkelen van een adapter om de database uit te lezen. Deze was nog niet goed genoeg. Ik heb het uitgebreid met een pipeline die de data opdeelt in de evenementen waarin ze opgeslagen worden een aan de hand van de timestamps aan elkaar verbonden. Dit resulteert in een net dataframe met minimale NaN-waarden. Hierna was ik begonnen aan de exploratory data analyse. Hieruit kwamen de volgende inzichten. Veel outliers. Correlatie verschil tussen ruimten. Gemeten data verschilt per ruimte. Volgende week wil ik eventuele oplossingen voor deze uitdagingen bespreken met met mijn technisch begeleider.","title":"Week 3"},{"location":"blog/#week-4","text":"01-03-2021 t/m 05-03-2021 Deze week is er een nieuwe sprint begonnen. Het doel van deze sprint is om de data requirements op te stellen, de data te verzamelen en voor te bereiden om machine learning modellen te trainen. Deze week is aan de volgende onderdelen gewerkt; Story mapping, data requirements onderzoek en projectplan Story mapping Vorige week was ik begonnen om de story mapping sessie voor te bereiden. Deze week was er een afspraak gepland om deze sessie te houden. Samen met Jeroen & Marco van Techtenna hebben we besproken wat de richting van het project gaat worden. De bedoeling was dat hier een complete backlog uit kwam. Momenteel zijn het alleen nog \"epics\". Volgende week maandag wil ik bespreken welke concrete taken hierbij horen. Data requirements onderzoek Aan de hand van de exploratory data analyse heb ik een data requirements analyse uitgevoerd. Hierbij heb ik gekeken of verschillende modellen die moeten gaan worden ontwikkeld worden al door andere zijn ontwikkeld. Hiervoor heb ik een viertal raporrten gevonden waarop ik een \"data-boodschappenlijst\" heb kunnen samenstellen. De komende week hoop ik alle items van dit lijstje af te kunnen strepen. Projectplan Op maandag had ik feedback ontvangen over mijn projectplan van Bartosz. Dit had ik opgedeeld in onderdelen zodat ik op donderdag of vrijdag alle onderdelen verwerkt zou hebben. Ik heb mezelf aan deze planning kunnen houden en de tweede versie van het plan is nu af. Deze ga ik op sturen voor feedback of goedkeuring. Volgende week verwacht ik dit afgerond te hebben.","title":"Week 4"},{"location":"blog/#week-5","text":"08-03-2021 t/m 12-03-2021 Deze week ben ik begonnen met het verzamelen van data uit externe bronnen. Daarnaast heb ik het projectplan afgerond. Dataverzameling Vorige week was ik ge\u00ebindigd met het samenstellen van een \"data-boodschappenlijstje\". Het doel was alle items deze week te verzamelen. In de meeste gevallen is dit gelukt. Zo heb ik bijvoorbeeld een bouwtekening van het kantoor ontvangen waaruit ik de oppervlakte van de ruimten heb gehaald. Er is veel meteorologische data benodigd. De grootste bron hiervan is het KNMI. Zij stellen een API beschikbaar waar accurate data over constante tijdsintervallen opgehaald kunnen worden. Het probleem is dat er momenteel technische problemen zijn met deze API waardoor hij niet beschikbaar is. Een oplossing die ik hier voor gevonden heb ik om historische data te downloaden en koppelen aan de data de vanuit Twindle verzameld wordt. Voor nu werkt dit en kan ik verder met het ontwikkelen van de machine learning modellen. Machine learning modellen Aangezien de data verzameling voorspoediger verliep dan verwacht kon ik een week eerder aan de machine learning model ontwikkeling beginnen. Ik heb er voor gekozen om te beginnen met relatief simpele lineaire regressie modellen. Hiermee heb ik al accurate resultaten kunnen bereiken zoals in de onderstaande afbeelding te zien valt. Volgende week wil ik voor de andere meetwaarden dezelfde techniek toepassen en kijken of het net zo goed werkt. Projectplan Deze week was de deadline voor het projectplan. Bartosz had nog wat kleine aanmerkingen. Deze heb ik verwerkt en zal ik volgende week of de week daarop bespreken.","title":"Week 5"},{"location":"blog/#week-6","text":"15-03-2021 t/m 19-03-2021 Deze week was er een nieuwe sprint begonnen. Het doel van deze sprint is om een eerste prototype van de modellen op te leveren. Linear Regression Vorige week ben ik begonnen met het ontwikkelen van linear regression modellen. De eerste indruk was dat deze goed presteerde. Ik was wat achterdochtig over de 0.997 R2 score die de modellen behaalde en heb dat verder onderzocht. Het bleek dat de data seizoensgebonden en autocorrelaties bevat. Dit maakt traditionele regressie modellen ongeschikt. Zoals in de bovenstaande afbeelding resulteerd dit soort data in modellen die altijd de waarden van t-1 weergeven. In R2 scores en dergelijke lijkt het te werken maar uiteindelijk bevat het model geen voorspellende capaciteiten. Time Series Prediction Na wat onderzoek te hebben gedaan ben ik uit gekomen op time series prediction. Dit is iets waar ik nog nooit eerder mee heb gewerkt en wat aardig complex is. Momenteel heb ik de data verder onderzocht op seizoensgebondenheid, autocorrelatie en trends. Vanuit hier wil ik volgende week met technieken zoals ARIMA aan de slag om modellen te ontwikkelen.","title":"Week 6"},{"location":"blog/#week-7","text":"22-03-2021 t/m 26-03-2021 vorige week was ik tot de conclusie gekomen dat mijn lineare regressie modellen geen voorspellende waarde hadden. Deze week heb ik verder onderzoek gedaan naar time series prediction en de volgende drie model soorten uitgeprobeerd; Linear regression, Exponential Smoothing & ARIMA. Voor ik begin met de experimenten had ik vastgesteld dat modellen beoordeeld worden op R2 score. Een model moet een minimale R2 score van 0.90 halen om als goed beschouwd te worden. Linear Regression Omdat de data erg sterke autocorrelaties bevat heb ik de keuze gemaakt om in plaats van de echte waarde te voorspellen het verschil tussen twee punten te voorspellen. De modellen die ik hier mee ontwikkeld heb komen niet boven de 0.30 R2 score uit en zijn dus niet goed genoeg. Waarschijnlijk mis ik nog belangrijke features die leiden tot een verandering in de meetwaarde. Exponential Smoothing & ARIMA Deze technieken werkte beide redelijk goed maar haalde het gestelde doel niet. Cross-validation moet toegepast worden om te bepalen hoe goed het model echt presteert. Volgende week wil ik beginnen met het ontwikkelen van de integratie van de modellen in Twindle. Hiervoor zal ik beginnen met een ontwerp maken wat ik daarna bespreek met mijn technisch begeleider. Parallel hieraan wil ik de modellen verder verbeteren. De eerste stap hierin is om meer data te verzamelen die invloed kan hebben op veranderingen in de meetwaarde.","title":"Week 7"},{"location":"blog/#week-8","text":"29-03-2021 t/m 02-04-2021 Deze week was er een nieuwe sprint begonnen. Deze sprint stond in het teken van het ontwerpen en ontwikkelen van de integratie van de machine learning modellen in de bestaande Twindle applicatie. Ontwerp De koppeling van de machine learning modellen is ontworpen volgend het C4 model. Op deze manier kan van een hoog tot laag abstractie niveau gewerkt worden. Het belangrijkste aan dit ontwerp is dat de machine learning modellen perdiodiek ge\u00fcpdatet moeten worden met nieuwe data. Hiervoor is goed nagedacht over de communicatie met de API van de Twindle applicatie. De eerste versie van het ontwerp is afgerond. Volgende week zal ik met mijn technisch begeleider dit ontwerp bespreken, aanpassingen maken en beginnen met de implementatie. Planning Het plan was om deze sprint de applicatie te ontwerpen en te implementeren. Waarschijnlijk is deze planning te optimistisch. Zoals het er nu voor staat zal er volgende week donderdag begonnen worden aan de implementatie. Een extra sprint zal benodigd zijn om deze af te ronden. Tijdens het opstellen van het project plan is rekening gehouden dat deze situatie zich voor kon doen.","title":"Week 8"},{"location":"blog/#week-9","text":"05-04-2021 t/m 02-04-2021 Vorige week was vastgesteld dat er een extra sprint benodigd was om de applicatie te kunnen implementeren. Deze week is verder gewerkt aan het ontwerp. Daarnaast is er een oplevering geweest voor Techtenna waar een verandering van het doel is besproken. Ontwerp Vorige week was een groot gedeelte van het ontwerp afgerond. Deze week is dit met Samet besproken. Er moest nog meer verduidelijkt worden over de database, het was nog niet duidelijk wat voor soort database toegepast ging worden en met welke technieken. Hier is verder onderzoek naar gedaan waarvan de conclusie was dat er een MySQL database gebruikt gaat worden. Momenteel is er een andere stagiaire, Dmitri, ook bezig aan uitbreidingen van de Twindle applicatie. Samen met hem is het ontwerp besproken om ervoor te zorgen dat we elkaar niet tegenwerken. Oplevering & doelwijziging Na aanleiding van de storymapping sessie in week vier was er besloten iedere maand een update te geven aan de personen van Techtenna. Hiervoor was er een presentatie voorbereid waarin de bevindingen van de machine learning experimenten en het ontwerp werden besproken. Tijdens het ontwerpen van de applicatie stuiten ik op meerdere artikelen waarin verteld werd dat een groot gedeelte van de machine learning projecten nooit afgerond word. Dit komt doordat het proces wat gebruikt wordt om machine learning modellen te ontwikkelen niet per se goed aansluit bij software- en DevOps processen. Hierdoor had ik voorgesteld om in plaats van voor de drie service lagen modellen te ontwikkelen de focus te leggen op de luchtkwaliteit modellen. Zo kan ik een systeem ontwikkelen wat ervoor zal zorgen dat de machine learning modellen continu worden ge\u00fcpdatet met de nieuwste data en bijgestuurd kunnen worden waar nodig. Dit voorstel werd positief ontvangen door mijn stagebegeleiders en Marco van Techtenna. Het ontwerp is nu afgerond en goedgekeurd en zal de komende twee weken worden geimplementeerd.","title":"Week 9"},{"location":"blog/#week-10","text":"12-04-2021 t/m 16-04-2021 Vorige week was het ontwerp van mijn applicatie goedgekeurd. Het bestaat uit drie onderdelen, ML API, Twindle front-end en Twindle back-end. Deze week is gewerkt aan het eerste onderdeel, de ML API. Pipeline Het proces wat nu in de notebooks gebruikt wordt om de ARIMA modellen te maken moest worden omgezet naar een geautomatiseerde pipeline. Hiervoor heb ik een modules gemaakt die data kan opvragen uit het Twindle back-end, deze data kunnen voorbereiden en de modellen kunnen updaten en daarna opslaan in een database. Hierbij liep ik tegen het probleem aan dat de .pkl bestanden ~1.5 GB per stuk waren. Dit bleek te groot te zijn om in een SQL database op te slaan. Na wat onderzoek te doen en mogelijkheden te bespreken heb ik besloten om het pad naar de bestanden op te slaan i.p.v. de complete modellen. API De forecasts die gemaakt worden na het updaten van de modellen moeten kunnen worden opgevraagd via een API. Hiervoor heb ik de Flask library toegepast. Ik verwachte dat dit een redelijk simpel proces was aangezien ik maar 1 endpoint hoefde te ontwikkelen. Aangezien ik niet veel ervaring heb met het ontwikkelen van applicaties in Python ging dit wat minder soepel dan verwacht. Uiteindelijk is het me gelukt om dit endpoint toe te voegen. De manier waarop ben ik alleen nog niet helemaal tevreden over. Volgende week maandag heb ik een technische bespreking met Samet waarin ik, onder andere, dit wil aankaarten.","title":"Week 10"},{"location":"blog/#week-11","text":"19-04-2021 t/m 23-04-2021 Deze week heb ik samen met mijn technisch begeleider naar de ML pipeline en API gekeken. Hieruit kwam de volgende feedback: Pipeline meer SOLID opstellen Mappen structuur voor file opslag verbeteren Zorgen dat de code niet te lang wordt. Eerder onderbreken. Daarnaast vondt hij het goed dat ik veel comments heb geplaatst, gebruik maak van docstrings en een groot gedeelte van de applicatie automatisch getest kan worden via unit tests. Afonronding Pipeline Nadat de feedback verwerkt was ben ik verder gegaan met het ontwikkelen van de pipeline. Voornamelijk heb ik aan het systeem gewerkt waardoor er steeds een tijdsinterval naar voor wordt verwerkt. Dit looped totdat er geen data meer beschikbaar is of tot dat het tijdstip waarnaar ge\u00fcpdatet dient te worden in de toekomst valt. Dit syteem werkt goed, ik wel gemerkt dat de voorspellingen altijd twee uur achterlopen. Waarschijnlijk komt dit doordat ik een fout heb gemaakt tijdens het berekenen van de datum. Volgende week ga ik dit probleem uitzoeken.","title":"Week 11"},{"location":"blog/#week-12","text":"28-04-2021 t/m 30-04-2021 Deze week heb ik een aantal dagen vrij genomen in verband met Koningsdag. Daardoor heb ik deze week maar drie dagen gewerkt. Deze week stond in het teken van de laatste bugs uit de ML pipeline halen. Bugs De volgende bugs zijn gevonden en opgelost. ARIMA Model werd geladen, ge\u00fcpdatet maar niet opgeslagen. Opgelost door model iedere iteratie op te slaan. Datum berekening liep een uur voor. Na deze veranderingen door te voeren heb ik het model ge\u00ebvalueerd. Hieruit blijkt dat de voorspellingen nog steeds een uur achter lopen. Het kan zijn dat dit komt door het model zelf en niet een fout in de pipeline. Als dit zo is zou dat betekenen dat dit model waarschijnlijk niet gaat werken voor dit systeem. Portfolio Tijdens de terugkomdag werd ons als tip meegegeven om iedere week een dag te besteden aan het portfolio. Naar aanleiding van deze feedback heb ik besloten om iedere vrijdag aan het portfolio te gaan werken. Deze week heb ik uitgezocht hoe ik MkDocs zo kan toepassen dat ik een online versie kan hebben en een versie die ik als bestanden kan inleveren. Daarnaast ben ik begonnen aan het schrijven van de leeswijzer.","title":"Week 12"},{"location":"blog/#week-13","text":"03-05-2021 t/m 07-05-2021 Deze week staat er op de planning om de eerste iteratie van de implementatie af te ronden. Deze week heb ik een aantal bugs opgelost uit de pipeline en het front-end uitgebreid met de voorspellingen die vanuit de pipeline worden gemaakt. Daarnaast heb ik deze week een oplevering gehad voor Techtenna. Pipeline Vorige week was ik begonnen met het het opslaan van de modellen na iedere iteratie. Hierdoor groeide de modellen erg snel in bestandsgrootte, 30GB +. Dit betekent dat de ARIMA modellen uit het pmdarima package niet bruikbaar zijn. Dit heb ik opgelost door ARIMA modellen uit het statsmodel package te gebruiken. Deze worden niet zo groot (~300 MB) en houden bij welke timestamps er gebruikt zijn. Zo kan ik beter bepalen met welke data het model ge\u00fcpdatet moet worden. Helaas blijft het probleem met de achterlopende voorspellingen bestaan. Hier ga ik verder op in tijdens de volgende onderwerpen. Front-end Ondanks dat de voorspellingen nog niet helemaal klopte wilde ik graag de front-end implementatie ontwikkelen. Dit kan ik namelijk meenemen tijdens de evaluatie van de modellen. Doordat ik de ML api goed heb getest ging de implementatie erg vlot. Oplevering Op woensdag heb ik een oplevering gehad voor Techtenna. Hieruit bleek dat de modellen waarschijnlijk niet goed gaan werken voor het probleem wat we proberen op te lossen. Marco adviseerde mij om feedback te vragen van een machine learning expert. Daarnaast verwacht hij dat we betere resultaten kunnen bereiken wanneer we meer data verzamelen die invloed hebben op de targets, bijvoorbeeld het aantal personen in een ruimte wat invloed heeft op de CO2 & TVOC waarde. Volgende maandag ga ik dit verder bespreken met mijn begeleiders. Daarnaast ga ik een afspraak inplannen met Bartosz om dit te bespreken.","title":"Week 13"},{"location":"blog/#week-14","text":"10-05-2021 14-05-2021 Na aanleiding van de oplevering vorige week heb ik met Samet besloten om eerst een model te ontwikkelen voor de CO2-waarde. Op deze manier heb ik genoeg tijd om een model te ontwikkelen, de pipeline aan te passen en de front-end implementatie af te ronden. Data Requirements Marco van Techtenna vertelde tijdens de oplevering vorige week dat het aantal personen in een ruimte veel invloed heeft op de CO2 gehalte. Na verder onderzoek hierna te doen kon ik dit bevestigen. Om deze data te verzamelen heb ik besloten om de Google Calendar API te gebruiken. Via deze API kan ik gemakkelijk opvragen hoeveel personen er aanwezig zijn bij een geplande afpsraak. Korte termijn model De eerste stap die ik heb genomen om het korte termijn model te ontwikkelen was bekijken of andere personen dit probleem al hebben opgelost. Hieruit kwam een erg goed onderzoek met een duidelijke uitleg, code en dataset. De stappen die hierin genomen worden heb ik nagemaakt met de Twindle dataset. Hieruit is een model gekomen wat erg accuraat kan voorspellen wat de CO2-waarde over twee minuten zal zijn. Volgende week wil ik een model ontwikkelen wat voor de lange termijn een globaal overzicht kan geven. Deze twee modellen kunnen dat gekoppeld worden in het front-end om een compleet overzicht te geven.","title":"Week 14"},{"location":"blog/#week-15","text":"17-05-2021 - 21-05-2021 Deze week is de planning om de tweede versie van de modellen af te ronden. Hiervoor moet nog een lange termijn model worden ontwikkelt. Lange termijn model In het onderzoek wat ik had gevonden voor het korte termijn model werdt gerefereerd naar een ander onderzoek wat op uurlijkse basis CO2 probeerde te voorspellen. Dit onderzoek heb ik doorgenomen. Helaas was het niet zo gedetailleerd als het andere onderzoek. Hierdoor heb ik zelf nog wat aanpassingen moeten maken aan het model. Uiteindelijk heb ik erg veel geprobeerd maar ben ik niet tot een juist model gekomen. Ik heb besloten om deze toch toe te passen, waarschijnlijk zou ik nog een complete stage periode bezig kunnen zijn met model experimenten uitvoeren. Ik vind het belangrijker dat ik uiteindelijk kan demonsteren dat het probleem opgelost wordt door mijn applicatie.","title":"Week 15"},{"location":"ontwerpen/","text":"Modelexperimenten versie 1 Voor de uitbreiding van het Twindleproject zijn machine learningmodellen toegepast om de luchtkwaliteit te voorspellen. Dit zijn modellen voor de luchtvochtigheid, temperatuur, het CO2- en TVOC-gehalte. Deze modellen moesten voldoen aan bepaalde eisen, welke besproken zijn in het analyseren: requirements - machine learning onderdeel. Beschrijving & resultaten Om een betere indicatie te krijgen wat voor soort modellen toegepast konden worden is extra data-exploratie verricht 1 . Hieruit is gebeleken dat de data seizoensgebonden- en niet stationair is. Dit betekend dat standaard least squares-regressie niet toegepast kon worden, dit leidt namelijk tot sterke overfitting. Na onderzoek 2 te hebben gedaan naar geschikte modelleringtechnieken is besloten om met de volgende modellen te experimenteren: Lineare Regressie 3 Exponential Smoothing 4 Autoregressive Integrated Moving Average 5 Lineaire Regressie Om lineare regressie toe te kunnen passen moest de data geen autocorrelaties bevatten en stationair zijn. Hiervoor waren differencing, het verschil tussen een meting en de voorgaande meting, technieken toegepast. Daarnaast was er een techniek 6 toegepast om de opeenvolgende data voor te bereiden voor gebruik in de algoritmen. De volgende modellen waren toegepast: LinearRegression (LR) KNearestNeigbors (KNN) RandomForestRegressor (RF) Afbeelding 1: Gemiddelde r-squared per meetwaarde en model Aan de bovenstaande afbeeldingen is het volgende af te leiden: Geen enkel linear regressiemodel behaalt de eis van 0.9 r-squared. De lage of negatieve r-squared scores betekenen dat er geen patronen gevonden waren. Waarschijnlijk missen er nog cruciale features. In verder iteraties is dit verder onderzocht. Exponential Smoothing Voorspelling die gemaakt worden met exponential smoothing methoden zijn gemiddelde van eerdere waarnemingen, waarbij de eerdere waarnemingen zwaarder meewegen dan de oudere. De Holt-Winters seasonal methode was toegepast om te kunnen profiteren van de trends en seizoensgebondenheid die aanwezig waren in de data. Het statsmodel package heeft hier de volgende implementatie van: ExponentialSmoothing Afbeelding 2: Exponential Smoothing resultaten per meetwaarde Dit soort modellen lijken op het begin de trend goed te volgen. Na ongeveer een of twee uur beginnen ze echter steeds minder accuraat te worden en overschieten ze vaak het doel. Voor meetwaarden die afhankelijk zijn van andere factoren, zoals luchtvochtigheid en temperatuur, zijn deze modellen minder accuraat. Autoregressive Integrated Moving Average Autoregressive integrated moving average (ARIMA) modellen maken gebruik van een combinatie van de volgende technieken. Autoregression (AR) Moving average (MA) Het kan gebruik maken van de trends, autocorrelatie en seizoensgebondenheid van de data. In dit geval is er gekozen voor de auto_arima methode van het PMDARIMA package. Hierbij worden meerdere combinaties van het model uitgetests en uiteindelijk het hoogst scorende model opgeslagen. Net als bij de Exponential Smoothing modellen presteerde deze techniek beter op de TVOC- en CO2-gehalte. Deze waarden hebben een wekelijkse seizoensgebondenheid die momenteel nog niet gemodelleerd is. Wanneer er meer data beschikbaar is zal dit meegenomen worden in het model, wat waarschijnlijk resulteren in hogere scores. Bevindingen Momenteel is er nog geen enkel model wat de gestelde eis van 0.9 r-squared score behaalt. Wel komen de ARIMA-modellen redelijk dicht in de buurt. Met deze reden is er voor gekozen om de eerste iteratie van de implementatie deze modellen toe te passen. Architectuur Ongeveer 90% van de data science projecten haalt het niet tot productie 7 . Dit komt doordat het proces wat gebruikt wordt om machine learningmodellen te ontwikkelen niet goed aansluit bij de software engineering & DevOps processen. Om dit te voorkomen bij het Digital Twin 3.0 project is hier rekening mee gehouden tijdens het ontwerp. Tijdens het ontwerp is er gewerkt met de C4 methoden 8 . Dit is een ontwerpraamwerk wat op vier abstractieniveaus de applicatie toelicht. Context; Op hoog niveau wordt weergegeven wie er op welke manier interacteert met de systemen. Container; Globaal overzicht van de software-architectuur. Focus op technologiekeuze en manier van communicatie. Component; Overzicht van welke onderdelen samen de container vormen. Code; Laag niveau overzicht van de code door, bijvoorbeeld, klassediagrammen. Vooronderzoek Voor dat een begin gemaakt was met het ontwerpen van de applicatie is eerst vooronderzoek 9 verricht om te bepalen welke tools en technieken toegepast gingen worden. Dit vooronderzoek ging dieper in op de volgende onderwerpen: Het opslaan en ophalen van de data en voorspellingen. Frameworks en tooling die hierbij passen. API; gekozen voor Flask Database; gekozen voor MySQL Hoe feedback verzameld wordt en iteratief verwerkt kan worden. Modellen periodiek trainen en evalueren (R 2 & RMSE) C1: Context In het onderstaande systeem contextdiagram zijn de gebruikersgroepen en de manier waarop zij de applicatie gebruiken gevisualiseerd. Afbeelding 3: Systeem contextdiagram C2: Containers In dit diagram wordt er verder ingezoomd op de twee softwaresystemen zoals deze zijn beschreven in het vorige diagram. Ze zijn verder onderverdeeld in containers. Containers zijn onderdelen van het systeem die apart van elkaar functioneren. In dit diagram ligt de focus op de technologie keuzes en manier van communicatie tussen containers. Afbeelding 4: Containerdiagram De webapplicatie haalt voorspellingen op uit de AI-layer. Om deze voorspellingen te kunnen maken is historische data nodig die voorzien wordt door de back-end applicatie van de Digital Twin. C3: Components Iedere container uit het vorige diagram bestaat uit een of meerdere componenten. In dit diagram zal de technologiekeuze, communicatie en verantwoordelijkheden van deze componenten worden toegelicht. Afbeelding 5: Componentdiagram C4: Code Onderstaand valt een overzicht te vinden van de routes die aan de API zijn toegevoegd. Via deze route kunnen de voorspellingen die gemaakt zijn opgevraagd worden door het front-end. Methode Route Parameters Type Response GET /forecasts/:location/:room/:metric [ { \"timestamp\": date, \"value\": float } .... ] In de applicatie zullen zich meerdere entiteiten bevinden. Onderstaand is een overzicht van deze entiteiten, hun attributen en relaties te zien. Afbeelding 6: Entity relationsdiagram Modelexperimenten versie 2 De voorgaande experimenten resulteerde nog niet in modellen die de eis van 0.9 r-squared haalde. Uiteindelijk is toch besloten om deze modellen toe te passen in de applicatie. Uit deze implementatie bleek dat de modellen niet presteerde zoals verwacht. Om het doel te behalen moeten de volgende aanpassingen gedaan worden: Onderzoeken wat invloed heeft op de veranderingen in de meetwaarden. Verder in de toekomst kijken Betere controle voor welk tijdstip de voorspelling wordt gemaakt. Bestandsgrootte limiteren om schaalbaarheid te vergroten In overleg met de opdrachtgevers en belangrijkste stakeholder, Techtenna, was besloten om eerst een model voor de CO2 waarde te ontwikkelen. Wanneer dit succesvol is bevonden kan dit worden uitgebreid naar de overige meetwaarden. Gegevensverzameling Tijdens de tweede oplevering van het project werd door Marco van Techtenna opgemerkt dat het aantal personen wat zich in een ruimte bevinden invloed heeft op de CO2-gehalten. Via de Google Calendar API is opgevraagd hoeveel personen zich op een bepaalde tijd in een ruimte bevinden. Deze gegevens zijn toegevoegd aan de dataset zoals beschreven in de vorige iteratie. Uit een vergelijkbaar onderzoek 10 is gebleken dat temperatuur, luchtvochtigheid en activiteitsniveau een mogelijke invloed kunnen hebben op het CO2-gehalte. Door deze waarden te visualiseren in een correlatieheatmap kon dit gevalideerd worden. Afbeelding 7: Correlatieheatmap Beschrijving & resultaten Om het tweede doel te behalen zijn er twee modellen uitgewerkt. Een voor lange termijn voorspellingen 11 en een voor korte termijn voorspellingen 12 . In het onderzoeksrapport 13 wordt verder toegelicht welke data toegepast is en hoe deze is voorbereid. Korte termijn - Modellen Tijdens onderzoek van Kallio et al. 10 worden meerdere mogelijke modellen beschreven. Deze zijn specifiek uitgekozen omdat ze accurate voorspellingen kunnen geven op auto-gecorreleerde data. Ridge Regression (RR) Decision Tree (DT) Random Forest (RF) Multilayer Perceptron (MLP) Aangezien het CO2-gehalte in een ruimte niet drastisch veranderd in korte tijd is er gekozen om als baseline een \u2018last-observation carried forward\u2019 model te gebruiken. Dit geeft het laatst gemeten CO2-gehalte aan als de voorspelde waarde. Korte termijn - Features Per model kan het verschillen welke features tot het beste resultaat leiden. Om dit te optimaliseren is gebruik gemaakt van de SelectKBest class. Deze klasse maakt gebruik van f_regression om te bepalen welke features het meest gecorreleerd zijn tot het target. Tussen de drie en dertig features zijn getest door middel van GridSearch . Onderstaand valt per model te zien welke features de hoogste score behalen. Afbeelding 8: Feature selectie Een belangrijke bevinding uit deze grafiek is dat de modellen niet alleen de voorgaande CO2 metingen gebruiken, wat overfitting zou veroorzaken, maar ook bijvoorbeeld verlichting- en luchtvochtigheidsniveaus. Korte termijn - Resultaten Door de voorspelde waarde tegenover daadwerkelijke waarde te plotten kunnen patronen in de fouten van model worden opgespoord. Ideaal gezien vormen deze plotten een diagonale lijn, zoals bij het baseline, RR en MLP model. Dit betekend dat de voorspelde waarde dicht bij de daadwerkelijke waarde ligt. Bij de DTR en RF modellen worden de voorspellingen onnauwkeurig wanneer ze hoger worden. Afbeelding 9: Daadwerkelijk vs. voorspelt De r-squared (R2) score van een model geeft aan hoe goed de fit is van een model. Hoe dichter dit bij 1.0 is hoe beter. Het MLP en RR model presteren beter dan de baseline. Dit valt ook te zien in de foutmarges van de modellen. Het RR model presteert het beste op RMSE en MAE Afbeelding 10: Model evaluatie Lange termijn - Resultaten Ieder model, baseline inbegrepen, vertoont hetzelfde patroon. Er zitten meer lage waarden in de dataset dan hoge. Hierdoor zijn alle modellen gebiased richting lagere voorspellingen. Afbeelding 9: Daadwerkelijk vs. voorspelt Bevindingen Vanuit de vorige iteratie waren er een viertal uitdagingen die opgelost moesten worden. Onderstaand zijn deze, en de oplossingen, toegelicht. Onderzoeken wat invloed heeft op veranderingen in de meetwaarden en deze data toevoegen aan de modellen. CO2-gehalten worden vooral bepaald door het aantal personen in een ruimte. Door gebruik te maken van de Google Calendar API kan opgevraagd worden hoeveel personen zich op een bepaalde tijd in de ruimte bevonden. Verder in de toekomst kijken, 3 uur of meer. In het onderzoek van Kallio et al. 10 werd geconcludeerd dat voorspellingen voor de lange termijn vaak niet accuraat zijn. Met deze reden is er gekozen om een model te ontwikkelen wat op korte termijn accuraat is, deze zal ondersteunt worden door een visualisatie van gemiddelde voor een bepaalde dag en tijd. Betere controleren voor welk tijdstip de voorspelling gemaakt worden. Door andere data voorbereidingstechnieken te gebruiken wordt er geen voorspelling gedaan voor een bepaald tijdstip. Hierdoor hoeft deze ook niet meer gecontroleerd te worden. Bestandsgrootte modellen verminderen om schaalbaarheid te vergroten. De ridge regressionmodellen zijn minder dan 64 KB groot. Dit betekend dat het makkelijk schaalbaar is wanneer ze in digital twins met veel ruimten moeten worden toegepast. Modelexperimenten rapport - versie 1: Gegevensoverzicht blz. 3 t/m 9 \u21a9 Modelexperimenten rapport - versie 1: Bronnen blz. 21 \u21a9 Lineare regressienotebook \u21a9 Exponential smoothingnotebook \u21a9 ARIMA-notebook \u21a9 Modelexperimenten rapport - versie 1: Gegevensvoorbereiding blz. 12 \u21a9 StackoverFlow Blog: How to put machine learning models into production \u21a9 c4model.com \u21a9 Ontwerpdocument: Vooronderzoek blz. 3 t/m 6 \u21a9 Forecasting office indoor CO2 concentration using machine learning with a one-year dataset \u21a9 \u21a9 \u21a9 Long-term notebook \u21a9 Short-term notebook \u21a9 Modelexperimenten rapport - versie 2: Model beschrijvingen blz. 5 t/m 6 \u21a9","title":"Ontwerpen"},{"location":"ontwerpen/#modelexperimenten-versie-1","text":"Voor de uitbreiding van het Twindleproject zijn machine learningmodellen toegepast om de luchtkwaliteit te voorspellen. Dit zijn modellen voor de luchtvochtigheid, temperatuur, het CO2- en TVOC-gehalte. Deze modellen moesten voldoen aan bepaalde eisen, welke besproken zijn in het analyseren: requirements - machine learning onderdeel.","title":"Modelexperimenten versie 1"},{"location":"ontwerpen/#beschrijving-resultaten","text":"Om een betere indicatie te krijgen wat voor soort modellen toegepast konden worden is extra data-exploratie verricht 1 . Hieruit is gebeleken dat de data seizoensgebonden- en niet stationair is. Dit betekend dat standaard least squares-regressie niet toegepast kon worden, dit leidt namelijk tot sterke overfitting. Na onderzoek 2 te hebben gedaan naar geschikte modelleringtechnieken is besloten om met de volgende modellen te experimenteren: Lineare Regressie 3 Exponential Smoothing 4 Autoregressive Integrated Moving Average 5 Lineaire Regressie Om lineare regressie toe te kunnen passen moest de data geen autocorrelaties bevatten en stationair zijn. Hiervoor waren differencing, het verschil tussen een meting en de voorgaande meting, technieken toegepast. Daarnaast was er een techniek 6 toegepast om de opeenvolgende data voor te bereiden voor gebruik in de algoritmen. De volgende modellen waren toegepast: LinearRegression (LR) KNearestNeigbors (KNN) RandomForestRegressor (RF) Afbeelding 1: Gemiddelde r-squared per meetwaarde en model Aan de bovenstaande afbeeldingen is het volgende af te leiden: Geen enkel linear regressiemodel behaalt de eis van 0.9 r-squared. De lage of negatieve r-squared scores betekenen dat er geen patronen gevonden waren. Waarschijnlijk missen er nog cruciale features. In verder iteraties is dit verder onderzocht. Exponential Smoothing Voorspelling die gemaakt worden met exponential smoothing methoden zijn gemiddelde van eerdere waarnemingen, waarbij de eerdere waarnemingen zwaarder meewegen dan de oudere. De Holt-Winters seasonal methode was toegepast om te kunnen profiteren van de trends en seizoensgebondenheid die aanwezig waren in de data. Het statsmodel package heeft hier de volgende implementatie van: ExponentialSmoothing Afbeelding 2: Exponential Smoothing resultaten per meetwaarde Dit soort modellen lijken op het begin de trend goed te volgen. Na ongeveer een of twee uur beginnen ze echter steeds minder accuraat te worden en overschieten ze vaak het doel. Voor meetwaarden die afhankelijk zijn van andere factoren, zoals luchtvochtigheid en temperatuur, zijn deze modellen minder accuraat. Autoregressive Integrated Moving Average Autoregressive integrated moving average (ARIMA) modellen maken gebruik van een combinatie van de volgende technieken. Autoregression (AR) Moving average (MA) Het kan gebruik maken van de trends, autocorrelatie en seizoensgebondenheid van de data. In dit geval is er gekozen voor de auto_arima methode van het PMDARIMA package. Hierbij worden meerdere combinaties van het model uitgetests en uiteindelijk het hoogst scorende model opgeslagen. Net als bij de Exponential Smoothing modellen presteerde deze techniek beter op de TVOC- en CO2-gehalte. Deze waarden hebben een wekelijkse seizoensgebondenheid die momenteel nog niet gemodelleerd is. Wanneer er meer data beschikbaar is zal dit meegenomen worden in het model, wat waarschijnlijk resulteren in hogere scores.","title":"Beschrijving &amp; resultaten"},{"location":"ontwerpen/#bevindingen","text":"Momenteel is er nog geen enkel model wat de gestelde eis van 0.9 r-squared score behaalt. Wel komen de ARIMA-modellen redelijk dicht in de buurt. Met deze reden is er voor gekozen om de eerste iteratie van de implementatie deze modellen toe te passen.","title":"Bevindingen"},{"location":"ontwerpen/#architectuur","text":"Ongeveer 90% van de data science projecten haalt het niet tot productie 7 . Dit komt doordat het proces wat gebruikt wordt om machine learningmodellen te ontwikkelen niet goed aansluit bij de software engineering & DevOps processen. Om dit te voorkomen bij het Digital Twin 3.0 project is hier rekening mee gehouden tijdens het ontwerp. Tijdens het ontwerp is er gewerkt met de C4 methoden 8 . Dit is een ontwerpraamwerk wat op vier abstractieniveaus de applicatie toelicht. Context; Op hoog niveau wordt weergegeven wie er op welke manier interacteert met de systemen. Container; Globaal overzicht van de software-architectuur. Focus op technologiekeuze en manier van communicatie. Component; Overzicht van welke onderdelen samen de container vormen. Code; Laag niveau overzicht van de code door, bijvoorbeeld, klassediagrammen. Vooronderzoek Voor dat een begin gemaakt was met het ontwerpen van de applicatie is eerst vooronderzoek 9 verricht om te bepalen welke tools en technieken toegepast gingen worden. Dit vooronderzoek ging dieper in op de volgende onderwerpen: Het opslaan en ophalen van de data en voorspellingen. Frameworks en tooling die hierbij passen. API; gekozen voor Flask Database; gekozen voor MySQL Hoe feedback verzameld wordt en iteratief verwerkt kan worden. Modellen periodiek trainen en evalueren (R 2 & RMSE) C1: Context In het onderstaande systeem contextdiagram zijn de gebruikersgroepen en de manier waarop zij de applicatie gebruiken gevisualiseerd. Afbeelding 3: Systeem contextdiagram C2: Containers In dit diagram wordt er verder ingezoomd op de twee softwaresystemen zoals deze zijn beschreven in het vorige diagram. Ze zijn verder onderverdeeld in containers. Containers zijn onderdelen van het systeem die apart van elkaar functioneren. In dit diagram ligt de focus op de technologie keuzes en manier van communicatie tussen containers. Afbeelding 4: Containerdiagram De webapplicatie haalt voorspellingen op uit de AI-layer. Om deze voorspellingen te kunnen maken is historische data nodig die voorzien wordt door de back-end applicatie van de Digital Twin. C3: Components Iedere container uit het vorige diagram bestaat uit een of meerdere componenten. In dit diagram zal de technologiekeuze, communicatie en verantwoordelijkheden van deze componenten worden toegelicht. Afbeelding 5: Componentdiagram C4: Code Onderstaand valt een overzicht te vinden van de routes die aan de API zijn toegevoegd. Via deze route kunnen de voorspellingen die gemaakt zijn opgevraagd worden door het front-end. Methode Route Parameters Type Response GET /forecasts/:location/:room/:metric [ { \"timestamp\": date, \"value\": float } .... ] In de applicatie zullen zich meerdere entiteiten bevinden. Onderstaand is een overzicht van deze entiteiten, hun attributen en relaties te zien. Afbeelding 6: Entity relationsdiagram","title":"Architectuur"},{"location":"ontwerpen/#modelexperimenten-versie-2","text":"De voorgaande experimenten resulteerde nog niet in modellen die de eis van 0.9 r-squared haalde. Uiteindelijk is toch besloten om deze modellen toe te passen in de applicatie. Uit deze implementatie bleek dat de modellen niet presteerde zoals verwacht. Om het doel te behalen moeten de volgende aanpassingen gedaan worden: Onderzoeken wat invloed heeft op de veranderingen in de meetwaarden. Verder in de toekomst kijken Betere controle voor welk tijdstip de voorspelling wordt gemaakt. Bestandsgrootte limiteren om schaalbaarheid te vergroten In overleg met de opdrachtgevers en belangrijkste stakeholder, Techtenna, was besloten om eerst een model voor de CO2 waarde te ontwikkelen. Wanneer dit succesvol is bevonden kan dit worden uitgebreid naar de overige meetwaarden.","title":"Modelexperimenten versie 2"},{"location":"ontwerpen/#gegevensverzameling","text":"Tijdens de tweede oplevering van het project werd door Marco van Techtenna opgemerkt dat het aantal personen wat zich in een ruimte bevinden invloed heeft op de CO2-gehalten. Via de Google Calendar API is opgevraagd hoeveel personen zich op een bepaalde tijd in een ruimte bevinden. Deze gegevens zijn toegevoegd aan de dataset zoals beschreven in de vorige iteratie. Uit een vergelijkbaar onderzoek 10 is gebleken dat temperatuur, luchtvochtigheid en activiteitsniveau een mogelijke invloed kunnen hebben op het CO2-gehalte. Door deze waarden te visualiseren in een correlatieheatmap kon dit gevalideerd worden. Afbeelding 7: Correlatieheatmap","title":"Gegevensverzameling"},{"location":"ontwerpen/#beschrijving-resultaten_1","text":"Om het tweede doel te behalen zijn er twee modellen uitgewerkt. Een voor lange termijn voorspellingen 11 en een voor korte termijn voorspellingen 12 . In het onderzoeksrapport 13 wordt verder toegelicht welke data toegepast is en hoe deze is voorbereid. Korte termijn - Modellen Tijdens onderzoek van Kallio et al. 10 worden meerdere mogelijke modellen beschreven. Deze zijn specifiek uitgekozen omdat ze accurate voorspellingen kunnen geven op auto-gecorreleerde data. Ridge Regression (RR) Decision Tree (DT) Random Forest (RF) Multilayer Perceptron (MLP) Aangezien het CO2-gehalte in een ruimte niet drastisch veranderd in korte tijd is er gekozen om als baseline een \u2018last-observation carried forward\u2019 model te gebruiken. Dit geeft het laatst gemeten CO2-gehalte aan als de voorspelde waarde. Korte termijn - Features Per model kan het verschillen welke features tot het beste resultaat leiden. Om dit te optimaliseren is gebruik gemaakt van de SelectKBest class. Deze klasse maakt gebruik van f_regression om te bepalen welke features het meest gecorreleerd zijn tot het target. Tussen de drie en dertig features zijn getest door middel van GridSearch . Onderstaand valt per model te zien welke features de hoogste score behalen. Afbeelding 8: Feature selectie Een belangrijke bevinding uit deze grafiek is dat de modellen niet alleen de voorgaande CO2 metingen gebruiken, wat overfitting zou veroorzaken, maar ook bijvoorbeeld verlichting- en luchtvochtigheidsniveaus. Korte termijn - Resultaten Door de voorspelde waarde tegenover daadwerkelijke waarde te plotten kunnen patronen in de fouten van model worden opgespoord. Ideaal gezien vormen deze plotten een diagonale lijn, zoals bij het baseline, RR en MLP model. Dit betekend dat de voorspelde waarde dicht bij de daadwerkelijke waarde ligt. Bij de DTR en RF modellen worden de voorspellingen onnauwkeurig wanneer ze hoger worden. Afbeelding 9: Daadwerkelijk vs. voorspelt De r-squared (R2) score van een model geeft aan hoe goed de fit is van een model. Hoe dichter dit bij 1.0 is hoe beter. Het MLP en RR model presteren beter dan de baseline. Dit valt ook te zien in de foutmarges van de modellen. Het RR model presteert het beste op RMSE en MAE Afbeelding 10: Model evaluatie Lange termijn - Resultaten Ieder model, baseline inbegrepen, vertoont hetzelfde patroon. Er zitten meer lage waarden in de dataset dan hoge. Hierdoor zijn alle modellen gebiased richting lagere voorspellingen. Afbeelding 9: Daadwerkelijk vs. voorspelt","title":"Beschrijving &amp; resultaten"},{"location":"ontwerpen/#bevindingen_1","text":"Vanuit de vorige iteratie waren er een viertal uitdagingen die opgelost moesten worden. Onderstaand zijn deze, en de oplossingen, toegelicht. Onderzoeken wat invloed heeft op veranderingen in de meetwaarden en deze data toevoegen aan de modellen. CO2-gehalten worden vooral bepaald door het aantal personen in een ruimte. Door gebruik te maken van de Google Calendar API kan opgevraagd worden hoeveel personen zich op een bepaalde tijd in de ruimte bevonden. Verder in de toekomst kijken, 3 uur of meer. In het onderzoek van Kallio et al. 10 werd geconcludeerd dat voorspellingen voor de lange termijn vaak niet accuraat zijn. Met deze reden is er gekozen om een model te ontwikkelen wat op korte termijn accuraat is, deze zal ondersteunt worden door een visualisatie van gemiddelde voor een bepaalde dag en tijd. Betere controleren voor welk tijdstip de voorspelling gemaakt worden. Door andere data voorbereidingstechnieken te gebruiken wordt er geen voorspelling gedaan voor een bepaald tijdstip. Hierdoor hoeft deze ook niet meer gecontroleerd te worden. Bestandsgrootte modellen verminderen om schaalbaarheid te vergroten. De ridge regressionmodellen zijn minder dan 64 KB groot. Dit betekend dat het makkelijk schaalbaar is wanneer ze in digital twins met veel ruimten moeten worden toegepast. Modelexperimenten rapport - versie 1: Gegevensoverzicht blz. 3 t/m 9 \u21a9 Modelexperimenten rapport - versie 1: Bronnen blz. 21 \u21a9 Lineare regressienotebook \u21a9 Exponential smoothingnotebook \u21a9 ARIMA-notebook \u21a9 Modelexperimenten rapport - versie 1: Gegevensvoorbereiding blz. 12 \u21a9 StackoverFlow Blog: How to put machine learning models into production \u21a9 c4model.com \u21a9 Ontwerpdocument: Vooronderzoek blz. 3 t/m 6 \u21a9 Forecasting office indoor CO2 concentration using machine learning with a one-year dataset \u21a9 \u21a9 \u21a9 Long-term notebook \u21a9 Short-term notebook \u21a9 Modelexperimenten rapport - versie 2: Model beschrijvingen blz. 5 t/m 6 \u21a9","title":"Bevindingen"},{"location":"planning/","text":"Planning Tijdens het project is er gewerkt in sprint van twee weken. Om het project te structureren is de IBM Data Science methodology toegepast worden, zie afbeelding 1: IBM Data Science Methodology . Per sprint is vermeldt worden onderzoeken en producten opgeleverd worden. Afbeelding 1: IBM Data Science Methodology # Fase Onderzoeken Producten Start Eind 0. Opstart Projectplan 08-02 12-02 1. 1 Exploratory data analyse Stakeholdersanalyse EDA Backlog 15-02 26-02 2. 2 Data requirements analyse Data requirements report Dataset 01-03 12-03 3. 3 - 4 Modellering experimenten Model prototype 15-03 26-03 4. 5 ML Pipeline ontwerp Ontwerpdocument 29-03 09-04 5. 5 Model integratie Prototype 12-04 23-04 6. 1 - 2 Model evaluatie EDA Dataset versie 2 26-04 07-05 7. 3 - 4 Model experimenten ML modellen versie 2 10-05 21-05 8. 24-05 04-06 9. 07-06 18-06","title":"Planning"},{"location":"planning/#planning","text":"Tijdens het project is er gewerkt in sprint van twee weken. Om het project te structureren is de IBM Data Science methodology toegepast worden, zie afbeelding 1: IBM Data Science Methodology . Per sprint is vermeldt worden onderzoeken en producten opgeleverd worden. Afbeelding 1: IBM Data Science Methodology # Fase Onderzoeken Producten Start Eind 0. Opstart Projectplan 08-02 12-02 1. 1 Exploratory data analyse Stakeholdersanalyse EDA Backlog 15-02 26-02 2. 2 Data requirements analyse Data requirements report Dataset 01-03 12-03 3. 3 - 4 Modellering experimenten Model prototype 15-03 26-03 4. 5 ML Pipeline ontwerp Ontwerpdocument 29-03 09-04 5. 5 Model integratie Prototype 12-04 23-04 6. 1 - 2 Model evaluatie EDA Dataset versie 2 26-04 07-05 7. 3 - 4 Model experimenten ML modellen versie 2 10-05 21-05 8. 24-05 04-06 9. 07-06 18-06","title":"Planning"},{"location":"realiseren/","text":"Manier van aanpak Tijdens het realiseren van het project is de scrum methode toegepast. Dit betekend dat er gewerkt is in sprints van twee weken. Aan het eind van deze twee weken zijn demo's gegeven aan de mede stagiairs en begeleiders. Daarnaast heeft er iedere vier weken een oplevering voor de belangrijkste stakeholders, Handpicked Labs & Techtenna plaatsgevonden. In de onderstaande secties zal per oplevering de geplande werkzaamheden, de gerealiseerde werkzaamheden, feedback en reflectie worden toegelicht. Oplevering 1: Ontwerp De eerste oplevering had vooral betrekking op het ontwerp van de applicatie en de machine learningmodellen. Meer details over het ontwerp kunnen gevonden worden op de \"Ontwerpen\" pagina. Geplande- en afgeronde werkzaamheden Sprint # Werkzaamheden Afgerond 3 Data requirementsanalyse Ja 3 Data verzameling Ja 4 Modelexperimenten Ja 4 Ontwerp Ja Resultaten Tijdens het ontwerpen van de applicatie werd duidelijk dat de modellen redelijk lastig waren om te ontwikkelen. Het oorspronkelijke doel was om voor alle service lagen, luchtkwaliteit, energiebesparing en brandveiligheid, modellen te ontwikkelen. Dit doel bleek niet realistisch te zijn. Om te voorkomen dat de machine learningmodellen falen in de productie omgeving was voorgesteld om het doel te beperken tot luchtkwaliteit. Hierdoor kon er meer aandacht besteed worden aan het ontwikkelen van een systeem wat periodiek de modellen update en evalueert. Wanneer dit goed werkt kan het uitgebreid worden naar de andere service lagen. Feedback Op het eind van de opleveringspresentatie 1 werd het voorstel gedaan om het doel aan te passen. De reacties hierop waren positief. De stakeholders van Handpicked Labs & Techtenna vonden het een goed idee om klein te beginnen, een robuust systeem te bouwen en daarna uit te breiden. Reflectie Het grootste gedeelte van deze sprints had ik besteed aan de experimenteren. Hierbij was het doel om voor vier verschillende meetwaarden een geschikt model te vinden. Om deze modellen te ontwikkelen waren verschillende technieken onderzocht 2 . Ik had wat moeite met geschikte modellen ontwikkelen, de ARIMA-modellen werkte uiteindelijk het beste maar het is een techniek waar ik nog nooit mee gewerkt had en het bleek lastiger dan verwacht. Na wat experimenten uit te voeren vond ik een goede bron voor dit type machine learning 3 . In de toekomst zou het beter zijn als ik eerst dit soort bronnen zoek voordat ik begin met experimenteren. Oplevering 2: Integratie ARIMA-modellen Vorige oplevering waren het ontwerp en doelswijzigvoorstel goedgekeurd. Deze oplevering was een eerste versie van dit ontwerp ge\u00efmplementeerd. Geplande- en afgeronde werkzaamheden Sprint # Werkzaamheden Afgerond 5 Pipeline implementatie Ja 6 Front-end aanpassingen Ja Resultaten In de onderstaande afbeelding is te zien hoe de gewenste uitkomst van de implementatie gepland was. In de linker afbeelding is de startsituatie te zien, dit is een visualisatie van de luchtvochtigheid van de afgelopen dag. In de gewenste eindsituatie wordt deze visualisatie uitgebreid met de verwachte luchtvochtigheid voor de komende uren. Afbeelding 1: Gewenste eindsituatie Om deze situatie te kunnen realiseren is de pipeline zoals deze in het ontwerp is beschreven ontwikkeld. Onderstaand is in een schematische tekening te zien hoe deze pipeline functioneert. Periodiek wordt er nieuwe data opgevraagd die voorbereid wordt om de ARIMA-modellen te updaten. Hierna worden voorspellingen en evaluaties gemaakt die opgeslagen worden in een database. Deze worden beschikbaar gesteld via een API om, bijvoorbeeld, in het front-end gevisualiseerd te worden. Afbeelding 2: Pipeline implementatie Onderstaand is te zien hoe deze voorspellingen verwerkt waren in het front-end. Het eind resultaat lijkt redelijk op de gewenste eindsituatie. Er zijn echter wat onderdelen die niet naar verwachting werkte. De voorspelling is slechts een uur in de toekomst. De ARIMA-modellen zijn erg groot (>1GB). De voorspellingen zijn niet accuraat genoeg. Afbeelding 3: Front-endimplementatie Feedback Tijdens de opleveringpresentatie 4 werd de onderstaande afbeelding getoond. Hierin is te zien dat de voorspellingen altijd ~1 uur achter lijken te lopen. Om dit op te lossen is veel tijd besteed aan het controleren met welke data de modellen werden ge\u00fcpdatet en voor welke tijdstippen de voorspellingen werden gemaakt. Uiteindelijk is er, mede door feedback van Marco van Techtenna, tot de conclusie gekomen dat dit een eigenschap is van de ARIMA-modellen. Tijdens de tweede modelexperimenten is deze feedback meegenomen. Afbeelding 4: ARIMA-evaluaties Andere feedback was dat de impact van de visualisatie niet sterk genoeg was om mensen aan te sporen de situatie te veranderen. Aangezien dit de kern van het probleem is hiervoor een tweede iteratie van de machine learningmodellen uitgevoerd. Reflectie Tijdens de 5e en 6e sprint was ik vooral bezig met het omzetten van de modelexperimenten naar een geautomatiseerde pipeline. Hierbij heb ik een API ontwikkelt en het front-end uitgebreid. Via deze API kan de front-endapplicatie voorspellingen ophalen om te visualiseren. De voorspellingen die uiteindelijk gevisualiseerd waren maakte niet de gewenste impact en waren niet accuraat genoeg. Hierdoor moest ik meer modelexperimenten uitvoeren. Daarnaast had ik voor het implementeren van de applicatie maar een sprint ingepland. Uiteindelijk had ik een complete sprint nodig om alleen de pipeline en API te ontwikkelen en een extra halve sprint om het front-end uit te breiden. Hierdoor verwachte ik in tijdsnood zou kunnen komen. Oplevering 3: Iteratie op integratie Na aanleiding van de feedback op de tweede iteratie waren nieuwe modelexperimenten uitgevoerd. Deze modellen zijn gebruikt om de pipeline en de front-endapplicatie aan te passen. Geplande werkzaamheden Sprint # Werkzaamheden Afgerond 7 Modelexperimenten Ja 8 Pipeline aanpassingen Ja 8 Front-end aanpassingen Ja 8 Meldingen weergeven Nee Gerealiseerde werkzaamheden Voordat begonnen was aan het implementeren van de nieuwe modellen in de pipeline en front-end was er een mock-up gemaakt van de gewenste eindsituatie. Afbeelding 5: Mock-up Door de korte termijn voorspellingen een aparte balkgrafiek te visualiseren en de kleur aan te passen wanneer bepaalde waarden worden overschreden wordt er een grotere impact gemaakt. Op deze manier zal de gebruiker aangespoord worden om in te grijpen wanneer de situatie dreigt te verslechteren. De aanpassingen die aan de pipeline gemaakt moesten worden waren minimaal. Over het algemeen kon de applicatie versimpeld worden omdat het machine learningmodel niet afhankelijk is van timestamps . Onderstaand is de front-endimplementatie van de mock-up van afbeelding 5 te zien. De realisatie was succesvol, er zijn echter nog wat verbeter punten: Datum objecten worden niet goed vertaald naar labels Horizontale belijning is hardcoded. Afbeelding 6: Mock-up realisatie Feedback Tijdens de presentatie 5 waren de meningen erg positief. De stakeholders vonden dat de feedback die voorgaande oplevering gegeven was goed verwerkt was. De manier waarop de voorspelling worden gevisualiseerd is op deze manier beter en spoort aan om in te grijpen wanneer de situatie dreigt te verslechteren. Reflectie Gedurende de zevende en achtste sprints ben ik bezig geweest met modelexperimenten uitvoeren en de resulterende modellen verwerken in de pipeline en front-endapplicatie. Hierbij was het doel om de drie ge\u00efdentificeerd problemen op te lossen. Om niet opnieuw het wiel uit te vinden, iets wat de voorgaande iteratie wel het geval was, was ik begonnen met het zoeken naar vergelijkbare oplossingen voor het machine learningmodel. Hierdoor kon ik gerichter werken, wat resulteerde in een beter machine learningmodel. Ik ben erg tevreden met deze manier van werken en zal dit in de toekomst vaker toepassen. Het verwerken van het model in de pipeline ging vlot. Ik had er rekening mee gehouden dat er veranderingen konden plaatsvinden en kon hier snel op inspelen. Wel is het nog redelijk veel werk om modellen te verwijderen of te vervangen. Wanneer dit product echt gebruikt gaat worden zal hier een oplossing voor gevonden moeten worden. De front-endimplementatie verliep niet zo soepel. Ik ben erg lang bezig geweest om met Chart.js informatieve grafieken te maken. Aangezien ik hier nog geen ervaring mee had ging het erg moeizaam. Daarnaast ging door een update aan het package de date parsing kapot. Momenteel heb ik dit handmatig opgelost maar dit resulteert nog niet in de gewenste visualisaties. Oplevering Twindle 3.0 Techtenna #1 07-02-21 \u21a9 Ontwerpen: Machine learning experimenten versie 1 \u21a9 Forecasting: Principles and Practice \u21a9 Oplevering Twindle 3.0 Techtenna #2 04-05-21 \u21a9 Oplevering Twindle 3.0 Techtenna # 3 02-06-21 \u21a9","title":"Realiseren"},{"location":"realiseren/#manier-van-aanpak","text":"Tijdens het realiseren van het project is de scrum methode toegepast. Dit betekend dat er gewerkt is in sprints van twee weken. Aan het eind van deze twee weken zijn demo's gegeven aan de mede stagiairs en begeleiders. Daarnaast heeft er iedere vier weken een oplevering voor de belangrijkste stakeholders, Handpicked Labs & Techtenna plaatsgevonden. In de onderstaande secties zal per oplevering de geplande werkzaamheden, de gerealiseerde werkzaamheden, feedback en reflectie worden toegelicht.","title":"Manier van aanpak"},{"location":"realiseren/#oplevering-1-ontwerp","text":"De eerste oplevering had vooral betrekking op het ontwerp van de applicatie en de machine learningmodellen. Meer details over het ontwerp kunnen gevonden worden op de \"Ontwerpen\" pagina.","title":"Oplevering 1: Ontwerp"},{"location":"realiseren/#geplande-en-afgeronde-werkzaamheden","text":"Sprint # Werkzaamheden Afgerond 3 Data requirementsanalyse Ja 3 Data verzameling Ja 4 Modelexperimenten Ja 4 Ontwerp Ja","title":"Geplande- en afgeronde werkzaamheden"},{"location":"realiseren/#resultaten","text":"Tijdens het ontwerpen van de applicatie werd duidelijk dat de modellen redelijk lastig waren om te ontwikkelen. Het oorspronkelijke doel was om voor alle service lagen, luchtkwaliteit, energiebesparing en brandveiligheid, modellen te ontwikkelen. Dit doel bleek niet realistisch te zijn. Om te voorkomen dat de machine learningmodellen falen in de productie omgeving was voorgesteld om het doel te beperken tot luchtkwaliteit. Hierdoor kon er meer aandacht besteed worden aan het ontwikkelen van een systeem wat periodiek de modellen update en evalueert. Wanneer dit goed werkt kan het uitgebreid worden naar de andere service lagen.","title":"Resultaten"},{"location":"realiseren/#feedback","text":"Op het eind van de opleveringspresentatie 1 werd het voorstel gedaan om het doel aan te passen. De reacties hierop waren positief. De stakeholders van Handpicked Labs & Techtenna vonden het een goed idee om klein te beginnen, een robuust systeem te bouwen en daarna uit te breiden.","title":"Feedback"},{"location":"realiseren/#reflectie","text":"Het grootste gedeelte van deze sprints had ik besteed aan de experimenteren. Hierbij was het doel om voor vier verschillende meetwaarden een geschikt model te vinden. Om deze modellen te ontwikkelen waren verschillende technieken onderzocht 2 . Ik had wat moeite met geschikte modellen ontwikkelen, de ARIMA-modellen werkte uiteindelijk het beste maar het is een techniek waar ik nog nooit mee gewerkt had en het bleek lastiger dan verwacht. Na wat experimenten uit te voeren vond ik een goede bron voor dit type machine learning 3 . In de toekomst zou het beter zijn als ik eerst dit soort bronnen zoek voordat ik begin met experimenteren.","title":"Reflectie"},{"location":"realiseren/#oplevering-2-integratie-arima-modellen","text":"Vorige oplevering waren het ontwerp en doelswijzigvoorstel goedgekeurd. Deze oplevering was een eerste versie van dit ontwerp ge\u00efmplementeerd.","title":"Oplevering 2: Integratie ARIMA-modellen"},{"location":"realiseren/#geplande-en-afgeronde-werkzaamheden_1","text":"Sprint # Werkzaamheden Afgerond 5 Pipeline implementatie Ja 6 Front-end aanpassingen Ja","title":"Geplande- en afgeronde werkzaamheden"},{"location":"realiseren/#resultaten_1","text":"In de onderstaande afbeelding is te zien hoe de gewenste uitkomst van de implementatie gepland was. In de linker afbeelding is de startsituatie te zien, dit is een visualisatie van de luchtvochtigheid van de afgelopen dag. In de gewenste eindsituatie wordt deze visualisatie uitgebreid met de verwachte luchtvochtigheid voor de komende uren. Afbeelding 1: Gewenste eindsituatie Om deze situatie te kunnen realiseren is de pipeline zoals deze in het ontwerp is beschreven ontwikkeld. Onderstaand is in een schematische tekening te zien hoe deze pipeline functioneert. Periodiek wordt er nieuwe data opgevraagd die voorbereid wordt om de ARIMA-modellen te updaten. Hierna worden voorspellingen en evaluaties gemaakt die opgeslagen worden in een database. Deze worden beschikbaar gesteld via een API om, bijvoorbeeld, in het front-end gevisualiseerd te worden. Afbeelding 2: Pipeline implementatie Onderstaand is te zien hoe deze voorspellingen verwerkt waren in het front-end. Het eind resultaat lijkt redelijk op de gewenste eindsituatie. Er zijn echter wat onderdelen die niet naar verwachting werkte. De voorspelling is slechts een uur in de toekomst. De ARIMA-modellen zijn erg groot (>1GB). De voorspellingen zijn niet accuraat genoeg. Afbeelding 3: Front-endimplementatie","title":"Resultaten"},{"location":"realiseren/#feedback_1","text":"Tijdens de opleveringpresentatie 4 werd de onderstaande afbeelding getoond. Hierin is te zien dat de voorspellingen altijd ~1 uur achter lijken te lopen. Om dit op te lossen is veel tijd besteed aan het controleren met welke data de modellen werden ge\u00fcpdatet en voor welke tijdstippen de voorspellingen werden gemaakt. Uiteindelijk is er, mede door feedback van Marco van Techtenna, tot de conclusie gekomen dat dit een eigenschap is van de ARIMA-modellen. Tijdens de tweede modelexperimenten is deze feedback meegenomen. Afbeelding 4: ARIMA-evaluaties Andere feedback was dat de impact van de visualisatie niet sterk genoeg was om mensen aan te sporen de situatie te veranderen. Aangezien dit de kern van het probleem is hiervoor een tweede iteratie van de machine learningmodellen uitgevoerd.","title":"Feedback"},{"location":"realiseren/#reflectie_1","text":"Tijdens de 5e en 6e sprint was ik vooral bezig met het omzetten van de modelexperimenten naar een geautomatiseerde pipeline. Hierbij heb ik een API ontwikkelt en het front-end uitgebreid. Via deze API kan de front-endapplicatie voorspellingen ophalen om te visualiseren. De voorspellingen die uiteindelijk gevisualiseerd waren maakte niet de gewenste impact en waren niet accuraat genoeg. Hierdoor moest ik meer modelexperimenten uitvoeren. Daarnaast had ik voor het implementeren van de applicatie maar een sprint ingepland. Uiteindelijk had ik een complete sprint nodig om alleen de pipeline en API te ontwikkelen en een extra halve sprint om het front-end uit te breiden. Hierdoor verwachte ik in tijdsnood zou kunnen komen.","title":"Reflectie"},{"location":"realiseren/#oplevering-3-iteratie-op-integratie","text":"Na aanleiding van de feedback op de tweede iteratie waren nieuwe modelexperimenten uitgevoerd. Deze modellen zijn gebruikt om de pipeline en de front-endapplicatie aan te passen.","title":"Oplevering 3: Iteratie op integratie"},{"location":"realiseren/#geplande-werkzaamheden","text":"Sprint # Werkzaamheden Afgerond 7 Modelexperimenten Ja 8 Pipeline aanpassingen Ja 8 Front-end aanpassingen Ja 8 Meldingen weergeven Nee","title":"Geplande werkzaamheden"},{"location":"realiseren/#gerealiseerde-werkzaamheden","text":"Voordat begonnen was aan het implementeren van de nieuwe modellen in de pipeline en front-end was er een mock-up gemaakt van de gewenste eindsituatie. Afbeelding 5: Mock-up Door de korte termijn voorspellingen een aparte balkgrafiek te visualiseren en de kleur aan te passen wanneer bepaalde waarden worden overschreden wordt er een grotere impact gemaakt. Op deze manier zal de gebruiker aangespoord worden om in te grijpen wanneer de situatie dreigt te verslechteren. De aanpassingen die aan de pipeline gemaakt moesten worden waren minimaal. Over het algemeen kon de applicatie versimpeld worden omdat het machine learningmodel niet afhankelijk is van timestamps . Onderstaand is de front-endimplementatie van de mock-up van afbeelding 5 te zien. De realisatie was succesvol, er zijn echter nog wat verbeter punten: Datum objecten worden niet goed vertaald naar labels Horizontale belijning is hardcoded. Afbeelding 6: Mock-up realisatie","title":"Gerealiseerde werkzaamheden"},{"location":"realiseren/#feedback_2","text":"Tijdens de presentatie 5 waren de meningen erg positief. De stakeholders vonden dat de feedback die voorgaande oplevering gegeven was goed verwerkt was. De manier waarop de voorspelling worden gevisualiseerd is op deze manier beter en spoort aan om in te grijpen wanneer de situatie dreigt te verslechteren.","title":"Feedback"},{"location":"realiseren/#reflectie_2","text":"Gedurende de zevende en achtste sprints ben ik bezig geweest met modelexperimenten uitvoeren en de resulterende modellen verwerken in de pipeline en front-endapplicatie. Hierbij was het doel om de drie ge\u00efdentificeerd problemen op te lossen. Om niet opnieuw het wiel uit te vinden, iets wat de voorgaande iteratie wel het geval was, was ik begonnen met het zoeken naar vergelijkbare oplossingen voor het machine learningmodel. Hierdoor kon ik gerichter werken, wat resulteerde in een beter machine learningmodel. Ik ben erg tevreden met deze manier van werken en zal dit in de toekomst vaker toepassen. Het verwerken van het model in de pipeline ging vlot. Ik had er rekening mee gehouden dat er veranderingen konden plaatsvinden en kon hier snel op inspelen. Wel is het nog redelijk veel werk om modellen te verwijderen of te vervangen. Wanneer dit product echt gebruikt gaat worden zal hier een oplossing voor gevonden moeten worden. De front-endimplementatie verliep niet zo soepel. Ik ben erg lang bezig geweest om met Chart.js informatieve grafieken te maken. Aangezien ik hier nog geen ervaring mee had ging het erg moeizaam. Daarnaast ging door een update aan het package de date parsing kapot. Momenteel heb ik dit handmatig opgelost maar dit resulteert nog niet in de gewenste visualisaties. Oplevering Twindle 3.0 Techtenna #1 07-02-21 \u21a9 Ontwerpen: Machine learning experimenten versie 1 \u21a9 Forecasting: Principles and Practice \u21a9 Oplevering Twindle 3.0 Techtenna #2 04-05-21 \u21a9 Oplevering Twindle 3.0 Techtenna # 3 02-06-21 \u21a9","title":"Reflectie"}]}